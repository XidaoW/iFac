{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/python\n",
    "# -*- coding:utf-8 -*-\n",
    "'''\n",
    "Created on 2019/01/02\n",
    "\n",
    "@author: xidaowen\n",
    "'''\n",
    "\n",
    "import ntf\n",
    "from myutil.histogram import createHistogram\n",
    "from myutil.plotter import showFactorValue, showHistDistribution\n",
    "from myutil.ponpare.reader import readPonpareData\n",
    "from myutil.ponpare.converter import     digitizeHistoryFeatureValue, transformForHistogram\n",
    "import multiview.mvtsne as mvtsne\n",
    "from sklearn.utils.testing import assert_raises\n",
    "\n",
    "\n",
    "import scipy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "# from scipy.special import entr\n",
    "from scipy import spatial\n",
    "\n",
    "import sys\n",
    "import json\n",
    "# from pyspark import SparkConf, SparkContext\n",
    "import itertools\n",
    "\n",
    "import logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "_log = logging.getLogger('JNTF')\n",
    "\n",
    "\n",
    "def showLabel(label):\n",
    "    for i1, lbl1 in enumerate(label):\n",
    "        print(\"label:[%d] ->\" % i1)\n",
    "        for lbl2 in lbl1:\n",
    "            print(lbl2 + \",\")\n",
    "        print(\"\")\n",
    "\n",
    "\n",
    "class iFacData():\n",
    "    def __init__(self):\n",
    "        self.domain = \"\"\n",
    "        self.labels = []\n",
    "        self.base = 0\n",
    "        self.cur_base = 0\n",
    "        self.hist = None\n",
    "        \n",
    "    def readData(self, domain = \"nba\", columns = []):\n",
    "        \"\"\"\n",
    "        read in the data and create labels\n",
    "        \"\"\"\n",
    "        self.domain = domain\n",
    "        if self.domain == \"nba\":\n",
    "            shots = pd.read_csv(\"data/NBA_shots_201415.csv\")\n",
    "            shots = shots[['PLAYER_ID','PLAYER_NAME','TEAM_ID','TEAM_NAME','ZoneName','PERIOD','SHOT_ATTEMPTED_FLAG','SHOT_MADE_FLAG']]\n",
    "            shots.PERIOD[shots.PERIOD > 4] = 5\n",
    "            self.column = ['PERIOD','TEAM_NAME','ZoneName']\n",
    "            shots_group_data_attempted = shots.groupby(self.column)['SHOT_ATTEMPTED_FLAG'].sum()\n",
    "            shots_group_data_attempted1 = shots_group_data_attempted.unstack(fill_value=0).to_panel()\n",
    "            self.hist = shots_group_data_attempted1.fillna(0).values\n",
    "            for i in range(len(self.column)):\n",
    "                each_label = shots_group_data_attempted1.fillna(0).axes[i].tolist()\n",
    "                each_label = [str(each_one).replace('!', '').replace('(','').replace(')','').replace(' ','') for each_one in each_label]\n",
    "                self.labels.append(each_label)\n",
    "            \n",
    "        if self.domain == \"nbaplayer\":\n",
    "            top_cnt = 15\n",
    "            shots = pd.read_csv(\"data/NBA_shots_201415.csv\")\n",
    "            shots = shots[['PLAYER_ID','PLAYER_NAME','TEAM_ID','TEAM_NAME','ZoneName','PERIOD','SHOT_ATTEMPTED_FLAG','SHOT_MADE_FLAG']]\n",
    "            shots.PERIOD[shots.PERIOD > 4] = 5\n",
    "\n",
    "            self.column = ['PERIOD','PLAYER_NAME','ZoneName']\n",
    "\n",
    "            shots_total = shots.groupby(['PLAYER_NAME'])['SHOT_ATTEMPTED_FLAG'].sum()\n",
    "            top_players = list(shots_total.sort_values(ascending=False).iloc[:top_cnt].index)\n",
    "\n",
    "            shots = shots[shots.PLAYER_NAME.isin(top_players)]\n",
    "            shots_group_data_attempted = shots.groupby(self.column)['SHOT_ATTEMPTED_FLAG'].sum()\n",
    "            shots_group_data_made = shots.groupby(self.column)['SHOT_MADE_FLAG'].sum()\n",
    "            shots_group_data_attempted = shots_group_data_made.div(shots_group_data_attempted, level=0)\n",
    "            shots_group_data_attempted1 = shots_group_data_attempted.unstack(fill_value=0).to_panel()\n",
    "            self.hist = shots_group_data_attempted1.fillna(0).values\n",
    "\n",
    "            for i in range(len(self.column)):\n",
    "                each_label = shots_group_data_attempted1.fillna(0).axes[i].tolist()\n",
    "                each_label = [str(each_one).replace('!', '').replace('(','').replace(')','').replace(' ','') for each_one in each_label]\n",
    "                self.labels.append(each_label)\n",
    "\n",
    "        elif self.domain == \"policy\":\n",
    "            policy = pd.read_csv(\"data/policy_adoption.csv\")\n",
    "            policy['adoption'] = 1\n",
    "            policy = policy[policy.adopted_year >= 1970]\n",
    "            policy = policy[policy.subject_name != \"Unknown\"]            \n",
    "            self.column = ['subject_name', 'adopted_year', 'state_id']\n",
    "            policy_group = policy.groupby(self.column)['adoption'].sum()\n",
    "            policy_group1 = policy_group.unstack(fill_value=0).to_panel()\n",
    "            self.hist = policy_group1.fillna(0).values\n",
    "            for i in range(len(self.column)):\n",
    "                each_label = policy_group1.fillna(0).axes[i].tolist()\n",
    "                each_label = [str(each_one).replace('!', '').replace('(','').replace(')','').replace(' ','') for each_one in each_label]\n",
    "                self.labels.append(each_label)            \n",
    "\n",
    "        elif self.domain == \"picso\":\n",
    "            policy = pd.read_csv(\"data/picso.csv\", header=None)\n",
    "            columns = ['member', 'year', 'keyword', 'value']\n",
    "            policy.columns = columns\n",
    "            self.column = columns[:3]\n",
    "            policy_group = policy.groupby(self.column)['value'].sum()\n",
    "            policy_group1 = policy_group.unstack(fill_value=0).to_panel()\n",
    "            self.hist = policy_group1.fillna(0).values\n",
    "            for i in range(len(self.column)):\n",
    "                each_label = policy_group1.fillna(0).axes[i].tolist()\n",
    "                each_label = [str(each_one) for each_one in each_label]\n",
    "                self.labels.append(each_label)  \n",
    "                \n",
    "                \n",
    "        elif self.domain == \"harvard\":                \n",
    "            harvard = pd.read_csv(\"/home/xidao/project/hipairfac/output/harvard_data_tensor_students.csv\")\n",
    "            columns = ['id', 'country', 'student', 'education','days','certified','grade','daysq']\n",
    "            harvard.columns = columns\n",
    "            self.column = ['student','country', 'education', 'daysq']\n",
    "            harvard_group = harvard.groupby(self.column)['certified'].sum()\n",
    "            harvard_group1 = harvard_group.unstack(fill_value=0).to_panel()\n",
    "            self.hist = harvard_group1.fillna(0).values\n",
    "            for i in range(len(self.column)):\n",
    "                each_label = harvard_group1.fillna(0).axes[i].tolist()\n",
    "                each_label = [str(each_one) for each_one in each_label]\n",
    "                self.labels.append(each_label)                  \n",
    "\n",
    "        elif self.domain == \"purchase\":\n",
    "            couponAreaTest, couponAreaTrain, couponDetailTrain,                 couponListTest, couponListTrain,                 couponVisitTrain, userList = readPonpareData(valuePrefixed=True)\n",
    "\n",
    "            # Convert to one-hot expression.\n",
    "            userList, couponListTrain, couponListTest =                 digitizeHistoryFeatureValue(userList,\n",
    "                                            couponListTrain,\n",
    "                                            couponListTest)\n",
    "            # Convert to histogram.\n",
    "            distribution = transformForHistogram(userList,\n",
    "                                                 couponDetailTrain,\n",
    "                                                 couponVisitTrain,\n",
    "                                                 couponListTrain,\n",
    "                                                 couponListTest,\n",
    "                                                 couponAreaTrain,\n",
    "                                                 couponAreaTest)\n",
    "            self.column = [\"SEX_ID\", \"GENRE_NAME\", \"LIST_PREF_NAME\",\"AGE\"]\n",
    "            self.hist, bins, label = createHistogram(distribution, self.column) \n",
    "            self.labels = [['00Male', '01Female'],\n",
    "                           ['00Gourmet', '01Este', '02Beauty', '03NailEye', '04HairSalon', \n",
    "                            '05HealthMedicalCare', '06Relaxation', '07Leisure', '08HotelInn',\n",
    "                            '09Lesson','10HomeDelivery','11GiftCard','12OtherCoupons'],\n",
    "                      ['00北海道', '01青森県', '02岩手県', '03宮城県', '04秋田県', '05山形県', '06福島県', '07茨城県', '08栃木県', '09群馬県', '10埼玉県', '11千葉県', \n",
    "                      '12東京都', '13神奈川県', '14新潟県', '15富山県', '16石川県', '17福井県', '18山梨県', '19長野県', '20岐阜県', '21静岡県', '22愛知県', '23三重県', \n",
    "                      '24滋賀県', '25京都府', '26大阪府', '27兵庫県', '28奈良県', '29和歌山県', '30鳥取県', '31島根県', '32岡山県', '33広島県', '34山口県', '35徳島県', \n",
    "                      '36香川県', '37愛媛県', '38高知県', '39福岡県', '40佐賀県', '41長崎県', '42熊本県', '43大分県', '44宮崎県', '45鹿児島県', '46沖縄県'],\n",
    "                      ['00under', '01-20', '02-25', '03-30', '04-35', '05-40', '06-45', '07-50', '08-55', '09-60', '10-65', '11-70', '12-75over'],\n",
    "                      ['00under', '01-100', '02-1000', '03-2000', '04-3000', '05-5000', '06-10000', '07-20000', '08-30000', '09-50000over\"']\n",
    "    \n",
    "                     ]            \n",
    "\n",
    "    def computeReconstructionError(self, ntfInstance, hist):    \n",
    "        \"\"\"\n",
    "        compute the reconstruction error\n",
    "        type ntfInstance: NTF:\n",
    "        type hist: np.array: tensor data\n",
    "        rtype error: float\n",
    "        \"\"\"\n",
    "        dstHist = ntfInstance.reconstruct()\n",
    "        srcHist = hist\n",
    "        diffHist = srcHist - dstHist\n",
    "        diffHistSum = np.sum(diffHist*diffHist)\n",
    "        srcHistSum = np.sum(srcHist*srcHist)\n",
    "        return diffHistSum/srcHistSum\n",
    "\n",
    "    def computeFit(self, ntfInstance, hist):\n",
    "        dstHist = ntfInstance.reconstruct()\n",
    "        mean_hist = np.full(hist.shape, np.mean(hist))\n",
    "        mean_hist_diff = (mean_hist - hist)\n",
    "        residual_hist = dstHist - hist\n",
    "        ss_total = np.sum(mean_hist_diff*mean_hist_diff)        \n",
    "        ss_res = np.sum(residual_hist*residual_hist)        \n",
    "        return 1 - ss_res*1. / ss_total\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def getFitForRanks(self, bases, trials = 5):\n",
    "        \"\"\"\n",
    "        compute the factors given different ranks and different random initializations\n",
    "        type bases: int: max number of components\n",
    "        type trials: int: number of independent trials\n",
    "        \"\"\"\n",
    "\n",
    "        def pctnonzero(arr):\n",
    "            return (len(arr) - np.count_nonzero(arr))*1./len(arr)\n",
    "\n",
    "        def gini(arr):\n",
    "            # (Warning: This is a concise implementation, but it is O(n**2)\n",
    "            # in time and memory, where n = len(x).  *Don't* pass in huge\n",
    "            # samples!)\n",
    "            # Mean absolute difference\n",
    "            mad = np.abs(np.subtract.outer(arr, arr)).mean()\n",
    "            # Relative mean absolute difference\n",
    "            rmad = mad/np.mean(arr)\n",
    "            # Gini coefficient\n",
    "            g = 0.5 * rmad\n",
    "            return g        \n",
    "\n",
    "        def normalized_entropy(arr):            \n",
    "            return stats.entropy(arr) *1. / np.log(len(arr))\n",
    "\n",
    "        def theil(arr): \n",
    "            # natural logarithm is default\n",
    "            redundancy = np.log(len(arr)) - stats.entropy(arr)\n",
    "            # inequality = 1 - exp(-redundancy)\n",
    "            return redundancy\n",
    "\n",
    "\n",
    "\n",
    "        self.base = bases\n",
    "        self.trials = trials\n",
    "        self.all_trials = []\n",
    "        self.metrics = {\"error\":[None]*self.base, \n",
    "                        \"fit\":[None]*self.base, \n",
    "                        \"stability\": [None]*self.base, \n",
    "                        \"entropy\": [None]*self.base, \n",
    "                        \"normalized_entropy\": [None]*self.base, \n",
    "                        \"pctnonzeros\": [None]*self.base, \n",
    "                        \"gini\": [None]*self.base, \n",
    "                        \"theil\": [None]*self.base, \n",
    "                        \"min_error_index\": [None]*self.base}\n",
    "        \n",
    "        self.weights_all = [None]*self.base\n",
    "        self.factors_all = [None]*self.base\n",
    "\n",
    "        conf = SparkConf().set(\"spark.driver.maxResultSize\", \"220g\").setAppName(\"DSGD_NTF\")\n",
    "        self.sc = SparkContext(conf=conf)\n",
    "        # def getNTF(random_seed, base_cnt, hist):\n",
    "        #   ntfInstance = ntf.NTF(base_cnt, hist, parallelCalc=True, ones = False, random_seed = random_seed)\n",
    "        #   ntfInstance.factorize(hist, showProgress=True)\n",
    "        #   # print(ntfInstance.factor)\n",
    "        #   return ntfInstance\n",
    "\n",
    "        # self.start_index = 2\n",
    "        for self.base_cnt in range(self.start_index, self.base+1):\n",
    "            try:\n",
    "                _log.info(\"Current Rank: {}\".format(self.base_cnt))\n",
    "                each_rank_trials = []\n",
    "                for random_seed in range(self.trials):\n",
    "                    _log.info(\"Current Trial: {}\".format(random_seed))\n",
    "                    ntfInstance = ntf.NTF(self.base_cnt, self.hist, parallelCalc=True, ones = False, random_seed = random_seed)\n",
    "                    ntfInstance.factorize(self.hist, showProgress=True)\n",
    "                    each_rank_trials.append(ntfInstance)\n",
    "\n",
    "                self.all_trials.append(each_rank_trials)\n",
    "                _log.info(\"Getting Metric for rank: {}\".format(self.base_cnt))\n",
    "                self.metrics[\"error\"][self.base_cnt-self.start_index] = []\n",
    "                self.metrics[\"fit\"][self.base_cnt-self.start_index] = []\n",
    "                self.metrics[\"stability\"][self.base_cnt-self.start_index] = []\n",
    "                self.metrics[\"entropy\"][self.base_cnt-self.start_index] = []            \n",
    "                self.metrics[\"normalized_entropy\"][self.base_cnt-self.start_index] = []\n",
    "                self.metrics[\"gini\"][self.base_cnt-self.start_index] = []            \n",
    "                self.metrics[\"theil\"][self.base_cnt-self.start_index] = []            \n",
    "                self.metrics[\"pctnonzeros\"][self.base_cnt-self.start_index] = []\n",
    "                self.weights_all[self.base_cnt-self.start_index] = []\n",
    "                self.factors_all[self.base_cnt-self.start_index] = []            \n",
    "                for random_seed in range(self.trials):\n",
    "                    _log.info(\"Getting Metric for Trial: {}\".format(random_seed))               \n",
    "                    ntfInstance = self.all_trials[self.base_cnt-self.start_index][random_seed]            \n",
    "                    self.metrics[\"error\"][self.base_cnt-self.start_index].append(self.computeReconstructionError(ntfInstance,self.hist))\n",
    "                    self.metrics[\"fit\"][self.base_cnt-self.start_index].append(self.computeFit(ntfInstance,self.hist))\n",
    "                    weights, factors = ntfInstance.getNormalizedFactor()\n",
    "                    self.weights_all[self.base_cnt-self.start_index].append(weights)\n",
    "                    self.factors_all[self.base_cnt-self.start_index].append(factors)\n",
    "                    self.metrics[\"entropy\"][self.base_cnt-self.start_index].append(np.mean([stats.entropy(factors[i][j]) for i in range(len(factors)) for j in range(len(factors[0]))]))\n",
    "                    self.metrics[\"normalized_entropy\"][self.base_cnt-self.start_index].append(np.mean([normalized_entropy(factors[i][j]) for i in range(len(factors)) for j in range(len(factors[0]))]))                \n",
    "                    self.metrics[\"pctnonzeros\"][self.base_cnt-self.start_index].append(np.mean([pctnonzero(factors[i][j]) for i in range(len(factors)) for j in range(len(factors[0]))]))\n",
    "                    self.metrics[\"theil\"][self.base_cnt-self.start_index].append(np.mean([theil(factors[i][j]) for i in range(len(factors)) for j in range(len(factors[0]))]))\n",
    "                    self.metrics[\"gini\"][self.base_cnt-self.start_index].append(np.mean([gini(factors[i][j]) for i in range(len(factors)) for j in range(len(factors[0]))]))\n",
    "                    \n",
    "\n",
    "                best_fit_index = np.argmin(self.metrics[\"error\"][self.base_cnt-self.start_index])\n",
    "                self.metrics[\"min_error_index\"][self.base_cnt-self.start_index] = int(best_fit_index)\n",
    "                self.best_factors = self.factors_all[self.base_cnt-self.start_index][best_fit_index]\n",
    "                self.best_weights = self.weights_all[self.base_cnt-self.start_index][best_fit_index]\n",
    "                for random_seed in range(self.trials):\n",
    "                    _log.info(\"Getting Similarity for Trial: {}\".format(random_seed))               \n",
    "                    self.cur_factors = self.factors_all[self.base_cnt-self.start_index][random_seed]\n",
    "                    self.cur_weights = self.weights_all[self.base_cnt-self.start_index][random_seed]\n",
    "                    self.metrics[\"stability\"][self.base_cnt-self.start_index].append(self.maxFactorSimilarity(self.cur_factors, self.cur_weights, self.best_factors, self.best_weights, self.base_cnt))   \n",
    "                self.cur_base = self.base_cnt                 \n",
    "                self.saveAttributes()\n",
    "            except:\n",
    "                # raise\n",
    "                continue\n",
    "                    \n",
    "\n",
    "    def maxFactorSimilarity(self, cur_factors, cur_weights, best_factors, best_weights, base_cnt):\n",
    "        \"\"\"\n",
    "        compute the max similarity to a given set of factors by permutations\n",
    "        based on equ.12 https://www.biorxiv.org/content/biorxiv/early/2017/10/30/211128.full.pdf\n",
    "        type cur_factors: array: the factors resulted from different runs\n",
    "        type cur_weights: array: the weights resulted from different runs\n",
    "        type best_factors: array: the factors with best fit\n",
    "        type best_weights: array: the weights with best fit\n",
    "        type base_cnt: int: the rank\n",
    "        rtype similarity: float: best similarity\n",
    "        \"\"\"\n",
    "        # from pprint import pprint\n",
    "        # import itertools\n",
    "        num_sample = 1000\n",
    "        # permuts = self.sc.parallelize(list(itertools.permutations(range(base_cnt)))).takeSample(False, num_sample, seed = 1)\n",
    "        random_seed = self.sc.parallelize(list(range(num_sample)))\n",
    "        \n",
    "\n",
    "\n",
    "        def computeEachSimilarity(each_seed, cur_factors, cur_weights, best_factors, best_weights):\n",
    "            each_permutation = list(np.random.RandomState(seed=each_seed).permutation(len(best_factors)))\n",
    "            # return np.mean([stats.spearmanr(cur_factors[list(each_permutation)[i]][j], best_factors[i][j])[0] for i in range(len(best_factors)) for j in range(len(best_factors[0]))])\n",
    "            similarity = 0.\n",
    "            for component_index in range(len(best_factors)):\n",
    "                rst = 1. - (abs(best_weights[component_index] - cur_weights[each_permutation[component_index]])) / max(best_weights[component_index], cur_weights[each_permutation[component_index]])\n",
    "                for factor_index in range(len(best_factors[0])):\n",
    "                    rst *= spatial.distance.cosine(cur_factors[each_permutation[component_index]][factor_index], best_factors[component_index][factor_index])\n",
    "                similarity += rst\n",
    "            similarity /= len(best_factors)\n",
    "\n",
    "            return similarity\n",
    "        \n",
    "        all_permutation_similarity = random_seed.map(lambda each_seed: computeEachSimilarity(each_seed, cur_factors, cur_weights, best_factors, best_weights)).collect()\n",
    "        similarity = max(all_permutation_similarity)\n",
    "        return similarity\n",
    "        \n",
    "            \n",
    "    def factorizeTensor(self, ones = True, random_seed = 1):\n",
    "        \"\"\"\n",
    "        factorize the tensor\n",
    "        type ones: boolean: whether use all ones as initialization\n",
    "        type random_seed: int: the random seed if not using ones\n",
    "        \"\"\"\n",
    "        \n",
    "        print(\"Start factorization...\")\n",
    "        self.ntfInstance = ntf.NTF(self.cur_base, self.hist, parallelCalc=True, ones = ones, random_seed = random_seed)\n",
    "        self.ntfInstance.factorize(self.hist, showProgress=True)\n",
    "        self.ntfInstance.normalizeFactor()        \n",
    "\n",
    "        \n",
    "    def normalizeFactor(self):\n",
    "        \"\"\"\n",
    "        normalize the weights\n",
    "        \"\"\"        \n",
    "        self.ntfInstance.normalizedWeight = self.ntfInstance.weight / np.sum(self.ntfInstance.weight)            \n",
    "    \n",
    "    def getFactors(self):\n",
    "        \"\"\"\n",
    "        obtain the factors\n",
    "        \"\"\"        \n",
    "        \n",
    "        self.factors = self.ntfInstance.factor\n",
    "#         self.column = ['ZONE','PERIOD', 'TEAM']\n",
    "        self.data = [np.array([self.factors[i][j].tolist() for i in range(len(self.factors))]) for j in range(len(self.column))]\n",
    "        \n",
    "\n",
    "    def computeItemSimilarity(self):\n",
    "        \"\"\"\n",
    "        compute the pairwise item similarity\n",
    "        \"\"\"\n",
    "        import math\n",
    "        self.itemSimilarity = {}\n",
    "        for k in range(len(self.data)):\n",
    "            self.itemSimilarity[k] = {}\n",
    "            for i in range(len(self.data[k].T)):\n",
    "                self.itemSimilarity[k][self.labels[k][i]] = {}\n",
    "                for j in range(len(self.data[k].T)):\n",
    "                    if i == j:\n",
    "                        continue\n",
    "                    dataSetI = self.data[k].T[i]\n",
    "                    dataSetII = self.data[k].T[j]\n",
    "                    # import pdb\n",
    "                    # pdb.set_trace()\n",
    "                    result = scipy.stats.spearmanr(dataSetI.T, dataSetII.T)\n",
    "                    # print(result)\n",
    "                    if not math.isnan(result.correlation):\n",
    "                        self.itemSimilarity[k][self.labels[k][i]][self.labels[k][j]] = result.correlation\n",
    "                    else:\n",
    "                        self.itemSimilarity[k][self.labels[k][i]][self.labels[k][j]] = 0\n",
    "\n",
    "                max_item = self.itemSimilarity[k][self.labels[k][i]][max(self.itemSimilarity[k][self.labels[k][i]], \n",
    "                                                                         key=self.itemSimilarity[k][self.labels[k][i]].get)]\n",
    "                min_item = self.itemSimilarity[k][self.labels[k][i]][min(self.itemSimilarity[k][self.labels[k][i]], \n",
    "                                                                         key=self.itemSimilarity[k][self.labels[k][i]].get)]\n",
    "                # normalize\n",
    "                if max_item != min_item:\n",
    "                    for j in self.itemSimilarity[k][self.labels[k][i]]:\n",
    "                        self.itemSimilarity[k][self.labels[k][i]][j] = (self.itemSimilarity[k][self.labels[k][i]][j] - min_item) / (max_item - min_item)\n",
    "\n",
    "    def computeEntropy(self):\n",
    "        \"\"\"\n",
    "        compute the entropy of each descriptor\n",
    "        \"\"\"\n",
    "        self.entropies = []\n",
    "        for j in range(len(self.factors[0])):\n",
    "            self.entropies.append([stats.entropy(self.factors[i][j]) for i in range(len(self.factors))])\n",
    "        self.max_entropy = np.max(self.entropies, axis = 1).tolist()\n",
    "        self.min_entropy = np.min(self.entropies, axis = 1).tolist()\n",
    "\n",
    "    def getMaxPatternForItem(self):\n",
    "        \"\"\"\n",
    "        compute the most relevant pattern for each item\n",
    "        \"\"\"\n",
    "        ## Get max pattern of each item\n",
    "        self.item_max_pattern = {}\n",
    "        for i in range(len(self.factors[0])):\n",
    "            self.item_max_pattern[i] = {}\n",
    "            for j in range(len(self.labels[i])):        \n",
    "                item_list_label = [self.factors[m][i][j] for m in range(len(self.factors))]        \n",
    "                self.item_max_pattern[i][self.labels[i][j]] = max(enumerate(item_list_label),key=lambda x: x[1])[0]\n",
    "        \n",
    "    def getMeanDistribution(self):\n",
    "        \"\"\"\n",
    "        compute the mean distribution of each descriptor\n",
    "        \"\"\"\n",
    "        data_mean = [np.mean([self.factors[i][j].tolist() for i in range(len(self.factors))],axis=0).tolist() for j in range(len(self.column))]\n",
    "        self.data_mean_descriptor = []\n",
    "        for m in range(len(data_mean)):\n",
    "            each_dict_descriptor = dict(zip(self.labels[m], data_mean[m]))\n",
    "            each_dict_descriptor['id'] = self.cur_base\n",
    "            self.data_mean_descriptor.append(each_dict_descriptor)\n",
    "        \n",
    "    def getEmbedding(self, rd_state = 3):\n",
    "        \"\"\"\n",
    "        use multiview tsne to embed the components to 2d plane\n",
    "        type rd_state: int: random state\n",
    "        \"\"\"\n",
    "        self.rd_state = rd_state\n",
    "        is_distance = [False] * len(self.data)\n",
    "        mvtsne_est = mvtsne.MvtSNE(k=2, perplexity = 10,random_state = self.rd_state, epoch = 3000)\n",
    "        mvtsne_est.fit(self.data, is_distance)\n",
    "        self.X_embedded = np.asarray(mvtsne_est.embedding_)        \n",
    "            \n",
    "            \n",
    "    def formatOutput(self):\n",
    "        self.data_output = {\"descriptors\": dict(zip(self.column, self.labels)),\n",
    "                            \"average\":self.data_mean_descriptor, \n",
    "                            \"itemSimilarity\":self.itemSimilarity,\n",
    "                            # \"metrics\":self.metrics,\n",
    "                            # \"item_max_pattern\": self.item_max_pattern,\n",
    "                            \"item_max_pattern\": '',\n",
    "                            \"start_index\":str(self.start_index),\n",
    "                            \"modes\": self.column}                \n",
    "        output = []\n",
    "        for i in range(len(self.factors)):\n",
    "            output_each = {}\n",
    "            output_each['id'] = i\n",
    "            output_each['factors'] = {}\n",
    "            output_each['dims'] = len(self.factors[i])\n",
    "            output_each['tsne_coord'] = {'x': self.X_embedded[i][0],'y':self.X_embedded[i][1]}\n",
    "            output_each['weight'] = self.ntfInstance.normalizedWeight[i]\n",
    "            output_each['max_tsne'] = np.max(self.X_embedded, axis = 0).tolist()\n",
    "            output_each['min_tsne'] = np.min(self.X_embedded, axis = 0).tolist()\n",
    "            for j in range(len(self.factors[i])):\n",
    "                a = self.factors[i][j]\n",
    "                output_each['factors'][j] = {}\n",
    "                output_each_factor = {}\n",
    "                output_each_factor['mode_id'] = j        \n",
    "                _dict = dict((self.labels[j][m], a[m]) for m in range(len(a)))\n",
    "                output_each_factor['max_item'] = max(_dict, key=_dict.get)\n",
    "                output_each_factor['min_item'] = min(_dict, key=_dict.get)\n",
    "                _dict['id'] = i\n",
    "                output_each_factor['values'] = _dict\n",
    "                output_each_factor['entropy'] = (self.entropies[j][i] - self.min_entropy[j]) / (self.max_entropy[j] - self.min_entropy[j])\n",
    "                output_each_factor['similarity'] = {}\n",
    "                for k in range(len(self.factors)):        \n",
    "                    if k == i:\n",
    "                        continue\n",
    "                    dataSetII = self.factors[k][j]\n",
    "                    dataSetI = self.factors[i][j]\n",
    "                    result = scipy.stats.spearmanr(dataSetI, dataSetII)[0]\n",
    "                    output_each_factor['similarity'][k] = result\n",
    "                dict_ = output_each_factor['similarity']\n",
    "                max_item = dict_[max(dict_, key=dict_.get)]\n",
    "                min_item = dict_[min(dict_, key=dict_.get)]\n",
    "                if max_item != min_item:\n",
    "                    for k in dict_:\n",
    "                        dict_[k] = (dict_[k] - min_item) / (max_item - min_item)\n",
    "                output_each_factor['similarity'] = dict_\n",
    "                output_each_factor['similarity']['average'] = sum(dict_.values())/len(dict_.values())  \n",
    "                output_each_factor['similarity']['max_idx'] = max(dict_, key=dict_.get)\n",
    "                output_each_factor['similarity']['min_idx'] = min(dict_, key=dict_.get)\n",
    "                output_each_factor['similarity'][i] = 1.0\n",
    "                output_each['factors'][j] = output_each_factor\n",
    "            output.append(output_each)\n",
    "\n",
    "        self.data_output[\"data\"] = output        \n",
    "            \n",
    "    def saveOutput(self):\n",
    "        \n",
    "        with open('/home/xidao/project/thesis/iFac/src/src/data/'+self.domain+'/factors_'+str(len(self.column))+'_'+str(self.cur_base)+'_sample_fit.json', 'w') as fp:\n",
    "            json.dump(self.data_output, fp)\n",
    "\n",
    "        with open('/home/xidao/project/thesis/iFac/src/src/data/'+self.domain+'/factors_'+str(len(self.column))+'_'+str(self.cur_base)+'_sample_fit_metrics.json', 'w') as fp:\n",
    "            json.dump(self.metrics, fp)         \n",
    "\n",
    "    def saveAttributes(self):\n",
    "        _log.info(\"Factorize Tensor\")   \n",
    "        self.factorizeTensor(ones = False, random_seed = iFac.metrics[\"min_error_index\"][self.cur_base-self.start_index])\n",
    "        _log.info(\"Get Factors\")          \n",
    "        self.normalizeFactor()\n",
    "        self.getFactors()\n",
    "        _log.info(\"Compute Item Similarity\")                    \n",
    "        self.computeItemSimilarity()\n",
    "        self.computeEntropy()\n",
    "        self.getMaxPatternForItem()\n",
    "        self.getMeanDistribution()\n",
    "        try:\n",
    "            self.getEmbedding()\n",
    "        except:\n",
    "            _log.info(\"running embedding again...\")\n",
    "            self.rd_state += 1\n",
    "            self.getEmbedding(rd_state = self.rd_state)\n",
    "        _log.info(\"Saving Output\")                              \n",
    "        self.formatOutput()\n",
    "        self.saveOutput()\n",
    "\n",
    "    def readFactorJSON(self, base_cnt=10, domain = \"\", ndims = 3):\n",
    "        self.base_cnt = base_cnt\n",
    "        self.domain = domain\n",
    "        self.ndims = ndims\n",
    "        file = \"../data/{}/factors_3_{}_sample_fit.json\".format(self.domain,self.ndims, self.base_cnt)\n",
    "        with open(file) as f:\n",
    "            self.data_output = json.load(f)\n",
    "\n",
    "    def readMetricJSON(self, base_cnt=10, domain = \"\", ndims = 3):\n",
    "        self.base_cnt = base_cnt\n",
    "        self.domain = domain\n",
    "        self.ndims = ndims\n",
    "        file = \"../data/{}/factors_{}_{}_sample_fit_metrics.json\".format(self.domain, self.ndims, self.base_cnt)\n",
    "        with open(file) as f:\n",
    "            metrics = json.load(f)\n",
    "        return metrics\n",
    "\n",
    "\n",
    "    def reEmbed(self, rd_state = 4):\n",
    "        self.getEmbedding(rd_state = rd_state)\n",
    "\n",
    "        for i in range(component_cnt):          \n",
    "            self.data_output['data'][i]['tsne_coord'] = {'x': self.X_embedded[i][0],'y':self.X_embedded[i][1]}\n",
    "            self.data_output['data'][i]['max_tsne'] = np.max(self.X_embedded, axis = 0).tolist()\n",
    "            self.data_output['data'][i]['min_tsne'] = np.min(self.X_embedded, axis = 0).tolist()\n",
    "\n",
    "        self.cur_base = component_cnt\n",
    "        self.column = self.data_output['modes']\n",
    "        self.saveOutput()\n",
    "\n",
    "\n",
    "def generateAll():\n",
    "    iFac = iFacData()\n",
    "    base = 30\n",
    "    iFac.start_index = 2\n",
    "    domain = \"policy\"   \n",
    "    nb_trials = 5\n",
    "\n",
    "    base = int(sys.argv[1])\n",
    "    iFac.start_index = int(sys.argv[2])\n",
    "    domain = str(sys.argv[3])\n",
    "\n",
    "    iFac.readData(domain = domain)\n",
    "    _log.info(\"Fitting Different Ranks up to {}\".format(base))\n",
    "    iFac.getFitForRanks(base, trials = nb_trials)\n",
    "\n",
    "def aggregateAll():\n",
    "    iFac = iFacData()\n",
    "    base = 30\n",
    "    iFac.start_index = 2\n",
    "    domain = \"policy\"   \n",
    "\n",
    "    iFac.start_index = int(sys.argv[1])\n",
    "    iFac.end_index = int(sys.argv[2])       \n",
    "    domain = str(sys.argv[3])\n",
    "        \n",
    "    start_metrics = iFac.readMetricJSON(base_cnt=iFac.start_index, domain = domain)\n",
    "    for i in range(iFac.start_index+1, iFac.end_index+1):\n",
    "        cur_metrics = iFac.readMetricJSON(base_cnt=i, domain = domain)\n",
    "        for m in measures:\n",
    "            cur_metrics[m] = [x for x in start_metrics[m] if x is not None] + [x for x in cur_metrics[m] if x is not None]        \n",
    "        with open('/home/xidao/project/thesis/iFac/src/src/data/'+domain+'/factors_3_'+str(i)+'_sample_fit_metrics.json', 'w') as fp:\n",
    "            json.dump(cur_metrics, fp)         \n",
    "\n",
    "def generateEmbedding():\n",
    "    return\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "# if __name__ == '__main__':\n",
    "#     generateAll()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "columns = ['id', 'country', 'student', 'education','days','certified','grade','daysq']\n",
    "harvard.columns = columns\n",
    "self.column = ['student','country', 'education', 'daysq']\n",
    "harvard_group = harvard.groupby(self.column)['certified'].sum()\n",
    "harvard_group1 = harvard_group.unstack(fill_value=0).to_panel()\n",
    "self.hist = harvard_group1.fillna(0).values\n",
    "for i in range(len(self.column)):\n",
    "    each_label = harvard_group1.fillna(0).axes[i].tolist()\n",
    "    each_label = [str(each_one) for each_one in each_label]\n",
    "    self.labels.append(each_label)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "iFac = iFacData()\n",
    "base = 6\n",
    "# iFac.start_index = \n",
    "# iFac.end_index = 40\n",
    "# domain = \"policy\"   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "iFac.readFactorJSON(base_cnt=iFac.start_index, domain = domain)\n",
    "# end_metrics = iFac.readMetricJSON(base_cnt=iFac.end_index, domain = domain)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'entropy': 1.0,\n",
       " 'max_item': '1994',\n",
       " 'min_item': '2015',\n",
       " 'mode_id': 2,\n",
       " 'similarity': {'0': 1.0,\n",
       "  '1': 0.0,\n",
       "  '2': 1.0,\n",
       "  'average': 0.5,\n",
       "  'max_idx': 2,\n",
       "  'min_idx': 1},\n",
       " 'values': {'1970': 0.01119217020125648,\n",
       "  '1971': 0.013496528905726805,\n",
       "  '1972': 0.018549014430579575,\n",
       "  '1973': 0.01119336240413476,\n",
       "  '1974': 0.02161236595111504,\n",
       "  '1975': 0.01559372964284481,\n",
       "  '1976': 0.018307434704253105,\n",
       "  '1977': 0.017682039353381716,\n",
       "  '1978': 0.01572192014897678,\n",
       "  '1979': 0.010444325664130094,\n",
       "  '1980': 0.027144955295703533,\n",
       "  '1981': 0.02676341226204817,\n",
       "  '1982': 0.04355101062496848,\n",
       "  '1983': 0.04811777997585326,\n",
       "  '1984': 0.04233758156882248,\n",
       "  '1985': 0.02896012072188075,\n",
       "  '1986': 0.04152991271052667,\n",
       "  '1987': 0.021713092951854456,\n",
       "  '1988': 0.035590816840206686,\n",
       "  '1989': 0.026677823252659015,\n",
       "  '1990': 0.027095995472599676,\n",
       "  '1991': 0.0192616241735752,\n",
       "  '1992': 0.026873036072370165,\n",
       "  '1993': 0.032818978953943964,\n",
       "  '1994': 0.055813725806120484,\n",
       "  '1995': 0.03854244702947875,\n",
       "  '1996': 0.05145465411708115,\n",
       "  '1997': 0.021532622538659157,\n",
       "  '1998': 0.04635265736115994,\n",
       "  '1999': 0.009299503661840134,\n",
       "  '2000': 0.03611250759770169,\n",
       "  '2001': 0.020878682337688238,\n",
       "  '2002': 0.044317721218795084,\n",
       "  '2003': 0.0190071910487131,\n",
       "  '2004': 0.011347958338133348,\n",
       "  '2005': 1.843677470857721e-07,\n",
       "  '2006': 0.04184557092327587,\n",
       "  '2007': 2.5261028249532756e-07,\n",
       "  '2008': 0.00017239859956273668,\n",
       "  '2009': 5.176441916577748e-07,\n",
       "  '2010': 0.0010917511439729659,\n",
       "  '2011': 1.4962527264938767e-10,\n",
       "  '2012': 6.212225593687226e-07,\n",
       "  '2013': 3.8188768101093856e-24,\n",
       "  '2014': 2.6543080105965836e-17,\n",
       "  '2015': 6.893080314757456e-28,\n",
       "  'id': 0}}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iFac.data_output['data'][0]['factors']['2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[34, 35, 36, 37, 38, 39, 40]"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(range(iFac.start_index+1, iFac.end_index+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "measures = [\"error\", \"fit\", \"stability\", \"entropy\", \"normalized_entropy\", \"pctnonzeros\", \"gini\", \"theil\", \"min_error_index\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# measures = [ \"normalized_entropy\", \"pctnonzeros\", \"gini\", \"theil\", \"min_error_index\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# measures = [ \"error\", \"fit\", \"stability\", \"entropy\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "53"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(cur_metrics[m][15:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# start_metrics['normalized_entropy']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(cur_metrics['pctnonzeros'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_metrics = iFac.readMetricJSON(base_cnt=19, domain = domain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_metrics == cur_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[3.070681571194888,\n",
       "  3.0858633619777596,\n",
       "  3.0694633385029966,\n",
       "  3.0744478059741684,\n",
       "  3.0685016663919025],\n",
       " [2.9701035188571208,\n",
       "  2.974732342414677,\n",
       "  2.972682795555583,\n",
       "  2.974940921403925,\n",
       "  2.9651898011057205],\n",
       " [2.8991880330872495,\n",
       "  2.9252764404817917,\n",
       "  2.880895768372488,\n",
       "  2.883548955164945,\n",
       "  2.8842413779795546],\n",
       " [2.8486468411470622,\n",
       "  2.837552126390974,\n",
       "  2.819043162209041,\n",
       "  2.8390590332832755,\n",
       "  2.832292734388924],\n",
       " [2.7777670902485236,\n",
       "  2.7881371484588144,\n",
       "  2.7900488280638776,\n",
       "  2.791062770653059,\n",
       "  2.797108910970279],\n",
       " [2.7142213815356464,\n",
       "  2.7381526772865863,\n",
       "  2.755655160207652,\n",
       "  2.7377779653907512,\n",
       "  2.7459659022978125],\n",
       " [2.6822463928321305,\n",
       "  2.69149247456135,\n",
       "  2.7028242665096216,\n",
       "  2.707157227277364,\n",
       "  2.7184430280207437],\n",
       " [2.6625181256154744,\n",
       "  2.6560536965926524,\n",
       "  2.667204025050188,\n",
       "  2.6736740921847373,\n",
       "  2.6712613325947347],\n",
       " [2.621718454003292,\n",
       "  2.6186268943455677,\n",
       "  2.6378625926476227,\n",
       "  2.641881020910946,\n",
       "  2.632900568353086],\n",
       " [2.5821867332586206,\n",
       "  2.587280217868835,\n",
       "  2.609459341334788,\n",
       "  2.5985709952770106,\n",
       "  2.5986000235525784],\n",
       " [2.5502927882076674,\n",
       "  2.5664460291804243,\n",
       "  2.564238883117591,\n",
       "  2.562218503945925,\n",
       "  2.561281010930049],\n",
       " [2.514608918654508,\n",
       "  2.515292699192051,\n",
       "  2.524130310042284,\n",
       "  2.5249874130850283,\n",
       "  2.5353232278095033],\n",
       " [2.487932631529025,\n",
       "  2.5013674445381833,\n",
       "  2.5139226516199553,\n",
       "  2.4917839420226477,\n",
       "  2.510076017981946],\n",
       " [2.4671621379704862,\n",
       "  2.462645822667161,\n",
       "  2.484519407352305,\n",
       "  2.46927255539827,\n",
       "  2.4631860933159526],\n",
       " [2.4314095174188917,\n",
       "  2.447878490874376,\n",
       "  2.4416696164935594,\n",
       "  2.4387318788513928,\n",
       "  2.4432000345883575],\n",
       " [2.4111744448789714,\n",
       "  2.4142835065086024,\n",
       "  2.3997035857592994,\n",
       "  2.4145572978097687,\n",
       "  2.4225103221899014],\n",
       " [2.396882664913085,\n",
       "  2.3995887415675807,\n",
       "  2.39471412085592,\n",
       "  2.3888495546254576,\n",
       "  2.3838582928263317],\n",
       " [2.365725725380589,\n",
       "  2.368459571540182,\n",
       "  2.3686156901302047,\n",
       "  2.360013822992312,\n",
       "  2.3715420442638724]]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cur_metrics['entropy']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[2.396882664913085,\n",
       "  2.3995887415675807,\n",
       "  2.39471412085592,\n",
       "  2.3888495546254576,\n",
       "  2.3838582928263317],\n",
       " [2.365725725380589,\n",
       "  2.368459571540182,\n",
       "  2.3686156901302047,\n",
       "  2.360013822992312,\n",
       "  2.3715420442638724],\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_metrics['entropy']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
