{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/python\n",
    "# -*- coding:utf-8 -*-\n",
    "'''\n",
    "Created on 2019/01/02\n",
    "\n",
    "@author: xidaowen\n",
    "'''\n",
    "\n",
    "import ntf\n",
    "from myutil.histogram import createHistogram\n",
    "from myutil.plotter import showFactorValue, showHistDistribution\n",
    "from myutil.ponpare.reader import readPonpareData\n",
    "from myutil.ponpare.converter import     digitizeHistoryFeatureValue, transformForHistogram\n",
    "import multiview.mvtsne as mvtsne\n",
    "from sklearn.utils.testing import assert_raises\n",
    "\n",
    "\n",
    "import scipy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "# from scipy.special import entr\n",
    "from scipy import spatial\n",
    "\n",
    "import sys\n",
    "import json\n",
    "# from pyspark import SparkConf, SparkContext\n",
    "import itertools\n",
    "# import xarray\n",
    "\n",
    "import logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "_log = logging.getLogger('JNTF')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "def showLabel(label):\n",
    "    for i1, lbl1 in enumerate(label):\n",
    "        print(\"label:[%d] ->\" % i1)\n",
    "        for lbl2 in lbl1:\n",
    "            print(lbl2 + \",\")\n",
    "        print(\"\")\n",
    "\n",
    "\n",
    "class iFacData():\n",
    "    def __init__(self):\n",
    "        self.domain = \"\"\n",
    "        self.labels = []\n",
    "        self.base = 0\n",
    "        self.cur_base = 0\n",
    "        self.hist = None\n",
    "        \n",
    "        \n",
    "    def createDataHistogram(self, dataFrame, extractColumn):\n",
    "        group = dataFrame.groupby(extractColumn).size()\n",
    "        index = group.index\n",
    "        hist = np.zeros(list(map(len, index.levels)))\n",
    "        for i1, pos in enumerate(zip(*index.labels)):\n",
    "            hist[pos] = group.values[i1]\n",
    "        labels = [list(i) for i in index.levels]\n",
    "        return hist, labels\n",
    "        \n",
    "    def readData(self, domain = \"nba\", columns = []):\n",
    "        \"\"\"\n",
    "        read in the data and create labels\n",
    "        \"\"\"\n",
    "        self.domain = domain\n",
    "        if self.domain == \"nba\":\n",
    "            shots = pd.read_csv(\"data/NBA_shots_201415.csv\")\n",
    "            shots = shots[['PLAYER_ID','PLAYER_NAME','TEAM_ID','TEAM_NAME','ZoneName','PERIOD','SHOT_ATTEMPTED_FLAG','SHOT_MADE_FLAG']]\n",
    "            shots.PERIOD[shots.PERIOD > 4] = 5\n",
    "            self.column = ['PERIOD','TEAM_NAME','ZoneName']\n",
    "            shots_group_data_attempted = shots.groupby(self.column)['SHOT_ATTEMPTED_FLAG'].sum()\n",
    "            shots_group_data_attempted1 = shots_group_data_attempted.unstack(fill_value=0).to_panel()\n",
    "            self.hist = shots_group_data_attempted1.fillna(0).values\n",
    "            for i in range(len(self.column)):\n",
    "                each_label = shots_group_data_attempted1.fillna(0).axes[i].tolist()\n",
    "                each_label = [str(each_one).replace('!', '').replace('(','').replace(')','').replace(' ','') for each_one in each_label]\n",
    "                self.labels.append(each_label)\n",
    "            \n",
    "        if self.domain == \"nbaplayer\":\n",
    "            top_cnt = 15\n",
    "            shots = pd.read_csv(\"data/NBA_shots_201415.csv\")\n",
    "            shots = shots[['PLAYER_ID','PLAYER_NAME','TEAM_ID','TEAM_NAME','ZoneName','PERIOD','SHOT_ATTEMPTED_FLAG','SHOT_MADE_FLAG']]\n",
    "            shots.PERIOD[shots.PERIOD > 4] = 5\n",
    "\n",
    "            self.column = ['PERIOD','PLAYER_NAME','ZoneName']\n",
    "\n",
    "            shots_total = shots.groupby(['PLAYER_NAME'])['SHOT_ATTEMPTED_FLAG'].sum()\n",
    "            top_players = list(shots_total.sort_values(ascending=False).iloc[:top_cnt].index)\n",
    "\n",
    "            shots = shots[shots.PLAYER_NAME.isin(top_players)]\n",
    "            shots_group_data_attempted = shots.groupby(self.column)['SHOT_ATTEMPTED_FLAG'].sum()\n",
    "            shots_group_data_made = shots.groupby(self.column)['SHOT_MADE_FLAG'].sum()\n",
    "            shots_group_data_attempted = shots_group_data_made.div(shots_group_data_attempted, level=0)\n",
    "            shots_group_data_attempted1 = shots_group_data_attempted.unstack(fill_value=0).to_panel()\n",
    "            self.hist = shots_group_data_attempted1.fillna(0).values\n",
    "\n",
    "            for i in range(len(self.column)):\n",
    "                each_label = shots_group_data_attempted1.fillna(0).axes[i].tolist()\n",
    "                each_label = [str(each_one).replace('!', '').replace('(','').replace(')','').replace(' ','') for each_one in each_label]\n",
    "                self.labels.append(each_label)\n",
    "\n",
    "        elif self.domain == \"policy\":\n",
    "            policy = pd.read_csv(\"data/policy_adoption.csv\")\n",
    "            policy['adoption'] = 1\n",
    "            policy = policy[policy.adopted_year >= 1970]\n",
    "            policy = policy[policy.subject_name != \"Unknown\"]            \n",
    "            self.column = ['subject_name', 'adopted_year', 'state_id']\n",
    "            policy_group = policy.groupby(self.column)['adoption'].sum()\n",
    "            policy_group1 = policy_group.unstack(fill_value=0).to_panel()\n",
    "            self.hist = policy_group1.fillna(0).values\n",
    "            for i in range(len(self.column)):\n",
    "                each_label = policy_group1.fillna(0).axes[i].tolist()\n",
    "                each_label = [str(each_one).replace('!', '').replace('(','').replace(')','').replace(' ','') for each_one in each_label]\n",
    "                self.labels.append(each_label)            \n",
    "\n",
    "        elif self.domain == \"picso\":\n",
    "            policy = pd.read_csv(\"data/picso.csv\", header=None)\n",
    "            columns = ['member', 'year', 'keyword', 'value']\n",
    "            policy.columns = columns\n",
    "            self.column = columns[:3]\n",
    "            policy_group = policy.groupby(self.column)['value'].sum()\n",
    "            policy_group1 = policy_group.unstack(fill_value=0).to_panel()\n",
    "            self.hist = policy_group1.fillna(0).values\n",
    "            for i in range(len(self.column)):\n",
    "                each_label = policy_group1.fillna(0).axes[i].tolist()\n",
    "                each_label = [str(each_one) for each_one in each_label]\n",
    "                self.labels.append(each_label)  \n",
    "\n",
    "        elif self.domain == \"biomarkder\":\n",
    "            policy = pd.read_csv(\"data/biomarker_gastric_complete.csv\", header=None)\n",
    "            columns = ['age', 'gender','type', 'protein', 'value']\n",
    "            policy.columns = columns\n",
    "            self.column = columns[:3]\n",
    "            policy_group = policy.groupby(self.column)['value'].sum()\n",
    "            policy_group1 = policy_group.unstack(fill_value=0).to_panel()\n",
    "            self.hist = policy_group1.fillna(0).values\n",
    "            for i in range(len(self.column)):\n",
    "                each_label = policy_group1.fillna(0).axes[i].tolist()\n",
    "                each_label = [str(each_one) for each_one in each_label]\n",
    "                self.labels.append(each_label)  \n",
    "                \n",
    "                \n",
    "        elif self.domain == \"harvard\":                \n",
    "            harvard = pd.read_csv(\"/home/xidao/project/hipairfac/output/harvard_data_tensor_students.csv\")\n",
    "            columns = ['id', 'country', 'student', 'education','days','certified','grade','daysq']\n",
    "            harvard.columns = columns\n",
    "            self.column = ['country', 'education', 'daysq', 'certified']\n",
    "            harvard = harvard[self.column]\n",
    "#             harvard_pos = harvard[harvard['certified'] == 1]\n",
    "#             harvard_neg = harvard[harvard['certified'] == 0]\n",
    "#             harvard_pos_sample = harvard_pos.sample(len(harvard_pos), replace=True, random_state = 1)\n",
    "#             harvard_neg_sample = harvard_neg.sample(len(harvard_pos), replace=True, random_state = 1)\n",
    "#             harvard_new = harvard_pos_sample.append(harvard_neg_sample)\n",
    "#             harvard_new = harvard_new.sample(frac=1).reset_index(drop=True)            \n",
    "            self.hist, self.labels = self.createDataHistogram(harvard_new, self.column)\n",
    "\n",
    "        elif self.domain == \"purchase\":\n",
    "            couponAreaTest, couponAreaTrain, couponDetailTrain,                 couponListTest, couponListTrain,                 couponVisitTrain, userList = readPonpareData(valuePrefixed=True)\n",
    "\n",
    "            # Convert to one-hot expression.\n",
    "            userList, couponListTrain, couponListTest =                 digitizeHistoryFeatureValue(userList,\n",
    "                                            couponListTrain,\n",
    "                                            couponListTest)\n",
    "            # Convert to histogram.\n",
    "            distribution = transformForHistogram(userList,\n",
    "                                                 couponDetailTrain,\n",
    "                                                 couponVisitTrain,\n",
    "                                                 couponListTrain,\n",
    "                                                 couponListTest,\n",
    "                                                 couponAreaTrain,\n",
    "                                                 couponAreaTest)\n",
    "            self.column = [\"SEX_ID\", \"GENRE_NAME\", \"LIST_PREF_NAME\",\"AGE\"]\n",
    "            self.hist, bins, label = createHistogram(distribution, self.column) \n",
    "            self.labels = [['00Male', '01Female'],\n",
    "                           ['00Gourmet', '01Este', '02Beauty', '03NailEye', '04HairSalon', \n",
    "                            '05HealthMedicalCare', '06Relaxation', '07Leisure', '08HotelInn',\n",
    "                            '09Lesson','10HomeDelivery','11GiftCard','12OtherCoupons'],\n",
    "                      ['00北海道', '01青森県', '02岩手県', '03宮城県', '04秋田県', '05山形県', '06福島県', '07茨城県', '08栃木県', '09群馬県', '10埼玉県', '11千葉県', \n",
    "                      '12東京都', '13神奈川県', '14新潟県', '15富山県', '16石川県', '17福井県', '18山梨県', '19長野県', '20岐阜県', '21静岡県', '22愛知県', '23三重県', \n",
    "                      '24滋賀県', '25京都府', '26大阪府', '27兵庫県', '28奈良県', '29和歌山県', '30鳥取県', '31島根県', '32岡山県', '33広島県', '34山口県', '35徳島県', \n",
    "                      '36香川県', '37愛媛県', '38高知県', '39福岡県', '40佐賀県', '41長崎県', '42熊本県', '43大分県', '44宮崎県', '45鹿児島県', '46沖縄県'],\n",
    "                      ['00under', '01-20', '02-25', '03-30', '04-35', '05-40', '06-45', '07-50', '08-55', '09-60', '10-65', '11-70', '12-75over'],\n",
    "                      ['00under', '01-100', '02-1000', '03-2000', '04-3000', '05-5000', '06-10000', '07-20000', '08-30000', '09-50000over\"']\n",
    "    \n",
    "                     ]            \n",
    "\n",
    "    def computeReconstructionError(self, ntfInstance, hist):    \n",
    "        \"\"\"\n",
    "        compute the reconstruction error\n",
    "        type ntfInstance: NTF:\n",
    "        type hist: np.array: tensor data\n",
    "        rtype error: float\n",
    "        \"\"\"\n",
    "        dstHist = ntfInstance.reconstruct()\n",
    "        srcHist = hist\n",
    "        diffHist = srcHist - dstHist\n",
    "        diffHistSum = np.sum(diffHist*diffHist)\n",
    "        srcHistSum = np.sum(srcHist*srcHist)\n",
    "        return diffHistSum/srcHistSum\n",
    "\n",
    "    def computeFit(self, ntfInstance, hist):\n",
    "        dstHist = ntfInstance.reconstruct()\n",
    "        mean_hist = np.full(hist.shape, np.mean(hist))\n",
    "        mean_hist_diff = (mean_hist - hist)\n",
    "        residual_hist = dstHist - hist\n",
    "        ss_total = np.sum(mean_hist_diff*mean_hist_diff)        \n",
    "        ss_res = np.sum(residual_hist*residual_hist)        \n",
    "        return 1 - ss_res*1. / ss_total\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def getFitForRanks(self, bases, trials = 5):\n",
    "        \"\"\"\n",
    "        compute the factors given different ranks and different random initializations\n",
    "        type bases: int: max number of components\n",
    "        type trials: int: number of independent trials\n",
    "        \"\"\"\n",
    "\n",
    "        def pctnonzero(arr):\n",
    "            return (len(arr) - np.count_nonzero(arr))*1./len(arr)\n",
    "\n",
    "        def gini(arr):\n",
    "            # (Warning: This is a concise implementation, but it is O(n**2)\n",
    "            # in time and memory, where n = len(x).  *Don't* pass in huge\n",
    "            # samples!)\n",
    "            # Mean absolute difference\n",
    "            mad = np.abs(np.subtract.outer(arr, arr)).mean()\n",
    "            # Relative mean absolute difference\n",
    "            rmad = mad/np.mean(arr)\n",
    "            # Gini coefficient\n",
    "            g = 0.5 * rmad\n",
    "            return g        \n",
    "\n",
    "        def normalized_entropy(arr):            \n",
    "            return stats.entropy(arr) *1. / np.log(len(arr))\n",
    "\n",
    "        def theil(arr): \n",
    "            # natural logarithm is default\n",
    "            redundancy = np.log(len(arr)) - stats.entropy(arr)\n",
    "            # inequality = 1 - exp(-redundancy)\n",
    "            return redundancy\n",
    "\n",
    "\n",
    "\n",
    "        self.base = bases\n",
    "        self.trials = trials\n",
    "        self.all_trials = []\n",
    "        self.metrics = {\"error\":[None]*self.base, \n",
    "                        \"fit\":[None]*self.base, \n",
    "                        \"stability\": [None]*self.base, \n",
    "                        \"entropy\": [None]*self.base, \n",
    "                        \"normalized_entropy\": [None]*self.base, \n",
    "                        \"pctnonzeros\": [None]*self.base, \n",
    "                        \"gini\": [None]*self.base, \n",
    "                        \"theil\": [None]*self.base, \n",
    "                        \"min_error_index\": [None]*self.base}\n",
    "        \n",
    "        self.weights_all = [None]*self.base\n",
    "        self.factors_all = [None]*self.base\n",
    "\n",
    "        conf = SparkConf().set(\"spark.driver.maxResultSize\", \"220g\").setAppName(\"DSGD_NTF\")\n",
    "        self.sc = SparkContext(conf=conf)\n",
    "        # def getNTF(random_seed, base_cnt, hist):\n",
    "        #   ntfInstance = ntf.NTF(base_cnt, hist, parallelCalc=True, ones = False, random_seed = random_seed)\n",
    "        #   ntfInstance.factorize(hist, showProgress=True)\n",
    "        #   # print(ntfInstance.factor)\n",
    "        #   return ntfInstance\n",
    "\n",
    "        # self.start_index = 2\n",
    "        for self.base_cnt in range(self.start_index, self.base+1):\n",
    "            try:\n",
    "                _log.info(\"Current Rank: {}\".format(self.base_cnt))\n",
    "                each_rank_trials = []\n",
    "                for random_seed in range(self.trials):\n",
    "                    _log.info(\"Current Trial: {}\".format(random_seed))\n",
    "                    ntfInstance = ntf.NTF(self.base_cnt, self.hist, parallelCalc=True, ones = False, random_seed = random_seed)\n",
    "                    ntfInstance.factorize(self.hist, showProgress=True)\n",
    "                    each_rank_trials.append(ntfInstance)\n",
    "\n",
    "                self.all_trials.append(each_rank_trials)\n",
    "                _log.info(\"Getting Metric for rank: {}\".format(self.base_cnt))\n",
    "                self.metrics[\"error\"][self.base_cnt-self.start_index] = []\n",
    "                self.metrics[\"fit\"][self.base_cnt-self.start_index] = []\n",
    "                self.metrics[\"stability\"][self.base_cnt-self.start_index] = []\n",
    "                self.metrics[\"entropy\"][self.base_cnt-self.start_index] = []            \n",
    "                self.metrics[\"normalized_entropy\"][self.base_cnt-self.start_index] = []\n",
    "                self.metrics[\"gini\"][self.base_cnt-self.start_index] = []            \n",
    "                self.metrics[\"theil\"][self.base_cnt-self.start_index] = []            \n",
    "                self.metrics[\"pctnonzeros\"][self.base_cnt-self.start_index] = []\n",
    "                self.weights_all[self.base_cnt-self.start_index] = []\n",
    "                self.factors_all[self.base_cnt-self.start_index] = []            \n",
    "                for random_seed in range(self.trials):\n",
    "                    _log.info(\"Getting Metric for Trial: {}\".format(random_seed))               \n",
    "                    ntfInstance = self.all_trials[self.base_cnt-self.start_index][random_seed]            \n",
    "                    self.metrics[\"error\"][self.base_cnt-self.start_index].append(self.computeReconstructionError(ntfInstance,self.hist))\n",
    "                    self.metrics[\"fit\"][self.base_cnt-self.start_index].append(self.computeFit(ntfInstance,self.hist))\n",
    "                    weights, factors = ntfInstance.getNormalizedFactor()\n",
    "                    self.weights_all[self.base_cnt-self.start_index].append(weights)\n",
    "                    self.factors_all[self.base_cnt-self.start_index].append(factors)\n",
    "                    self.metrics[\"entropy\"][self.base_cnt-self.start_index].append(np.mean([stats.entropy(factors[i][j]) for i in range(len(factors)) for j in range(len(factors[0]))]))\n",
    "                    self.metrics[\"normalized_entropy\"][self.base_cnt-self.start_index].append(np.mean([normalized_entropy(factors[i][j]) for i in range(len(factors)) for j in range(len(factors[0]))]))                \n",
    "                    self.metrics[\"pctnonzeros\"][self.base_cnt-self.start_index].append(np.mean([pctnonzero(factors[i][j]) for i in range(len(factors)) for j in range(len(factors[0]))]))\n",
    "                    self.metrics[\"theil\"][self.base_cnt-self.start_index].append(np.mean([theil(factors[i][j]) for i in range(len(factors)) for j in range(len(factors[0]))]))\n",
    "                    self.metrics[\"gini\"][self.base_cnt-self.start_index].append(np.mean([gini(factors[i][j]) for i in range(len(factors)) for j in range(len(factors[0]))]))\n",
    "                    \n",
    "\n",
    "                best_fit_index = np.argmin(self.metrics[\"error\"][self.base_cnt-self.start_index])\n",
    "                self.metrics[\"min_error_index\"][self.base_cnt-self.start_index] = int(best_fit_index)\n",
    "                self.best_factors = self.factors_all[self.base_cnt-self.start_index][best_fit_index]\n",
    "                self.best_weights = self.weights_all[self.base_cnt-self.start_index][best_fit_index]\n",
    "                for random_seed in range(self.trials):\n",
    "                    _log.info(\"Getting Similarity for Trial: {}\".format(random_seed))               \n",
    "                    self.cur_factors = self.factors_all[self.base_cnt-self.start_index][random_seed]\n",
    "                    self.cur_weights = self.weights_all[self.base_cnt-self.start_index][random_seed]\n",
    "                    self.metrics[\"stability\"][self.base_cnt-self.start_index].append(self.maxFactorSimilarity(self.cur_factors, self.cur_weights, self.best_factors, self.best_weights, self.base_cnt))   \n",
    "                self.cur_base = self.base_cnt                 \n",
    "                self.saveAttributes()\n",
    "            except:\n",
    "                # raise\n",
    "                continue\n",
    "                    \n",
    "\n",
    "    def maxFactorSimilarity(self, cur_factors, cur_weights, best_factors, best_weights, base_cnt):\n",
    "        \"\"\"\n",
    "        compute the max similarity to a given set of factors by permutations\n",
    "        based on equ.12 https://www.biorxiv.org/content/biorxiv/early/2017/10/30/211128.full.pdf\n",
    "        type cur_factors: array: the factors resulted from different runs\n",
    "        type cur_weights: array: the weights resulted from different runs\n",
    "        type best_factors: array: the factors with best fit\n",
    "        type best_weights: array: the weights with best fit\n",
    "        type base_cnt: int: the rank\n",
    "        rtype similarity: float: best similarity\n",
    "        \"\"\"\n",
    "        # from pprint import pprint\n",
    "        # import itertools\n",
    "        num_sample = 1000\n",
    "        # permuts = self.sc.parallelize(list(itertools.permutations(range(base_cnt)))).takeSample(False, num_sample, seed = 1)\n",
    "        random_seed = self.sc.parallelize(list(range(num_sample)))\n",
    "        \n",
    "\n",
    "\n",
    "        def computeEachSimilarity(each_seed, cur_factors, cur_weights, best_factors, best_weights):\n",
    "            each_permutation = list(np.random.RandomState(seed=each_seed).permutation(len(best_factors)))\n",
    "            # return np.mean([stats.spearmanr(cur_factors[list(each_permutation)[i]][j], best_factors[i][j])[0] for i in range(len(best_factors)) for j in range(len(best_factors[0]))])\n",
    "            similarity = 0.\n",
    "            for component_index in range(len(best_factors)):\n",
    "                rst = 1. - (abs(best_weights[component_index] - cur_weights[each_permutation[component_index]])) / max(best_weights[component_index], cur_weights[each_permutation[component_index]])\n",
    "                for factor_index in range(len(best_factors[0])):\n",
    "                    rst *= spatial.distance.cosine(cur_factors[each_permutation[component_index]][factor_index], best_factors[component_index][factor_index])\n",
    "                similarity += rst\n",
    "            similarity /= len(best_factors)\n",
    "\n",
    "            return similarity\n",
    "        \n",
    "        all_permutation_similarity = random_seed.map(lambda each_seed: computeEachSimilarity(each_seed, cur_factors, cur_weights, best_factors, best_weights)).collect()\n",
    "        similarity = max(all_permutation_similarity)\n",
    "        return similarity\n",
    "        \n",
    "            \n",
    "    def factorizeTensor(self, ones = True, random_seed = 1):\n",
    "        \"\"\"\n",
    "        factorize the tensor\n",
    "        type ones: boolean: whether use all ones as initialization\n",
    "        type random_seed: int: the random seed if not using ones\n",
    "        \"\"\"\n",
    "        \n",
    "        print(\"Start factorization...\")\n",
    "        self.ntfInstance = ntf.NTF(self.cur_base, self.hist, parallelCalc=True, ones = ones, random_seed = random_seed)\n",
    "        self.ntfInstance.factorize(self.hist, showProgress=True)\n",
    "        self.ntfInstance.normalizeFactor()        \n",
    "\n",
    "        \n",
    "    def normalizeFactor(self):\n",
    "        \"\"\"\n",
    "        normalize the weights\n",
    "        \"\"\"        \n",
    "        self.ntfInstance.normalizedWeight = self.ntfInstance.weight / np.sum(self.ntfInstance.weight)            \n",
    "    \n",
    "    def getFactors(self):\n",
    "        \"\"\"\n",
    "        obtain the factors\n",
    "        \"\"\"        \n",
    "        \n",
    "        self.factors = self.ntfInstance.factor\n",
    "#         self.column = ['ZONE','PERIOD', 'TEAM']\n",
    "        self.data = [np.array([self.factors[i][j].tolist() for i in range(len(self.factors))]) for j in range(len(self.column))]\n",
    "        \n",
    "\n",
    "    def computeItemSimilarity(self):\n",
    "        \"\"\"\n",
    "        compute the pairwise item similarity\n",
    "        \"\"\"\n",
    "        import math\n",
    "        self.itemSimilarity = {}\n",
    "        for k in range(len(self.data)):\n",
    "            self.itemSimilarity[k] = {}\n",
    "            for i in range(len(self.data[k].T)):\n",
    "                self.itemSimilarity[k][self.labels[k][i]] = {}\n",
    "                for j in range(len(self.data[k].T)):\n",
    "                    if i == j:\n",
    "                        continue\n",
    "                    dataSetI = self.data[k].T[i]\n",
    "                    dataSetII = self.data[k].T[j]\n",
    "                    # import pdb\n",
    "                    # pdb.set_trace()\n",
    "                    result = scipy.stats.spearmanr(dataSetI.T, dataSetII.T)\n",
    "                    # print(result)\n",
    "                    if not math.isnan(result.correlation):\n",
    "                        self.itemSimilarity[k][self.labels[k][i]][self.labels[k][j]] = result.correlation\n",
    "                    else:\n",
    "                        self.itemSimilarity[k][self.labels[k][i]][self.labels[k][j]] = 0\n",
    "\n",
    "                max_item = self.itemSimilarity[k][self.labels[k][i]][max(self.itemSimilarity[k][self.labels[k][i]], \n",
    "                                                                         key=self.itemSimilarity[k][self.labels[k][i]].get)]\n",
    "                min_item = self.itemSimilarity[k][self.labels[k][i]][min(self.itemSimilarity[k][self.labels[k][i]], \n",
    "                                                                         key=self.itemSimilarity[k][self.labels[k][i]].get)]\n",
    "                # normalize\n",
    "                if max_item != min_item:\n",
    "                    for j in self.itemSimilarity[k][self.labels[k][i]]:\n",
    "                        self.itemSimilarity[k][self.labels[k][i]][j] = (self.itemSimilarity[k][self.labels[k][i]][j] - min_item) / (max_item - min_item)\n",
    "\n",
    "    def computeEntropy(self):\n",
    "        \"\"\"\n",
    "        compute the entropy of each descriptor\n",
    "        \"\"\"\n",
    "        self.entropies = []\n",
    "        for j in range(len(self.factors[0])):\n",
    "            self.entropies.append([stats.entropy(self.factors[i][j]) for i in range(len(self.factors))])\n",
    "        self.max_entropy = np.max(self.entropies, axis = 1).tolist()\n",
    "        self.min_entropy = np.min(self.entropies, axis = 1).tolist()\n",
    "\n",
    "    def getMaxPatternForItem(self):\n",
    "        \"\"\"\n",
    "        compute the most relevant pattern for each item\n",
    "        \"\"\"\n",
    "        ## Get max pattern of each item\n",
    "        self.item_max_pattern = {}\n",
    "        for i in range(len(self.factors[0])):\n",
    "            self.item_max_pattern[i] = {}\n",
    "            for j in range(len(self.labels[i])):        \n",
    "                item_list_label = [self.factors[m][i][j] for m in range(len(self.factors))]        \n",
    "                self.item_max_pattern[i][self.labels[i][j]] = max(enumerate(item_list_label),key=lambda x: x[1])[0]\n",
    "        \n",
    "    def getMeanDistribution(self):\n",
    "        \"\"\"\n",
    "        compute the mean distribution of each descriptor\n",
    "        \"\"\"\n",
    "        data_mean = [np.mean([self.factors[i][j].tolist() for i in range(len(self.factors))],axis=0).tolist() for j in range(len(self.column))]\n",
    "        self.data_mean_descriptor = []\n",
    "        for m in range(len(data_mean)):\n",
    "            each_dict_descriptor = dict(zip(self.labels[m], data_mean[m]))\n",
    "            each_dict_descriptor['id'] = self.cur_base\n",
    "            self.data_mean_descriptor.append(each_dict_descriptor)\n",
    "        \n",
    "    def getEmbedding(self, rd_state = 3):\n",
    "        \"\"\"\n",
    "        use multiview tsne to embed the components to 2d plane\n",
    "        type rd_state: int: random state\n",
    "        \"\"\"\n",
    "        self.rd_state = rd_state\n",
    "        is_distance = [False] * len(self.data)\n",
    "        mvtsne_est = mvtsne.MvtSNE(k=2, perplexity = 10,random_state = self.rd_state, epoch = 3000)\n",
    "        mvtsne_est.fit(self.data, is_distance)\n",
    "        self.X_embedded = np.asarray(mvtsne_est.embedding_)        \n",
    "            \n",
    "            \n",
    "    def formatOutput(self):\n",
    "        self.data_output = {\"descriptors\": dict(zip(self.column, self.labels)),\n",
    "                            \"average\":self.data_mean_descriptor, \n",
    "                            \"itemSimilarity\":self.itemSimilarity,\n",
    "                            # \"metrics\":self.metrics,\n",
    "                            # \"item_max_pattern\": self.item_max_pattern,\n",
    "                            \"item_max_pattern\": '',\n",
    "                            \"start_index\":str(self.start_index),\n",
    "                            \"modes\": self.column}                \n",
    "        output = []\n",
    "        for i in range(len(self.factors)):\n",
    "            output_each = {}\n",
    "            output_each['id'] = i\n",
    "            output_each['factors'] = {}\n",
    "            output_each['dims'] = len(self.factors[i])\n",
    "            output_each['tsne_coord'] = {'x': self.X_embedded[i][0],'y':self.X_embedded[i][1]}\n",
    "            output_each['weight'] = self.ntfInstance.normalizedWeight[i]\n",
    "            output_each['max_tsne'] = np.max(self.X_embedded, axis = 0).tolist()\n",
    "            output_each['min_tsne'] = np.min(self.X_embedded, axis = 0).tolist()\n",
    "            for j in range(len(self.factors[i])):\n",
    "                a = self.factors[i][j]\n",
    "                output_each['factors'][j] = {}\n",
    "                output_each_factor = {}\n",
    "                output_each_factor['mode_id'] = j        \n",
    "                _dict = dict((self.labels[j][m], a[m]) for m in range(len(a)))\n",
    "                output_each_factor['max_item'] = max(_dict, key=_dict.get)\n",
    "                output_each_factor['min_item'] = min(_dict, key=_dict.get)\n",
    "                _dict['id'] = i\n",
    "                output_each_factor['values'] = _dict\n",
    "                output_each_factor['entropy'] = (self.entropies[j][i] - self.min_entropy[j]) / (self.max_entropy[j] - self.min_entropy[j])\n",
    "                output_each_factor['similarity'] = {}\n",
    "                for k in range(len(self.factors)):        \n",
    "                    if k == i:\n",
    "                        continue\n",
    "                    dataSetII = self.factors[k][j]\n",
    "                    dataSetI = self.factors[i][j]\n",
    "                    result = scipy.stats.spearmanr(dataSetI, dataSetII)[0]\n",
    "                    output_each_factor['similarity'][k] = result\n",
    "                dict_ = output_each_factor['similarity']\n",
    "                max_item = dict_[max(dict_, key=dict_.get)]\n",
    "                min_item = dict_[min(dict_, key=dict_.get)]\n",
    "                if max_item != min_item:\n",
    "                    for k in dict_:\n",
    "                        dict_[k] = (dict_[k] - min_item) / (max_item - min_item)\n",
    "                output_each_factor['similarity'] = dict_\n",
    "                output_each_factor['similarity']['average'] = sum(dict_.values())/len(dict_.values())  \n",
    "                output_each_factor['similarity']['max_idx'] = max(dict_, key=dict_.get)\n",
    "                output_each_factor['similarity']['min_idx'] = min(dict_, key=dict_.get)\n",
    "                output_each_factor['similarity'][i] = 1.0\n",
    "                output_each['factors'][j] = output_each_factor\n",
    "            output.append(output_each)\n",
    "\n",
    "        self.data_output[\"data\"] = output        \n",
    "            \n",
    "    def saveOutput(self):\n",
    "        \n",
    "        with open('/home/xidao/project/thesis/iFac/src/src/data/'+self.domain+'/factors_'+str(len(self.column))+'_'+str(self.cur_base)+'_sample_fit.json', 'w') as fp:\n",
    "            json.dump(self.data_output, fp)\n",
    "\n",
    "        with open('/home/xidao/project/thesis/iFac/src/src/data/'+self.domain+'/factors_'+str(len(self.column))+'_'+str(self.cur_base)+'_sample_fit_metrics.json', 'w') as fp:\n",
    "            json.dump(self.metrics, fp)         \n",
    "\n",
    "    def saveAttributes(self):\n",
    "        _log.info(\"Factorize Tensor\")   \n",
    "        self.factorizeTensor(ones = False, random_seed = iFac.metrics[\"min_error_index\"][self.cur_base-self.start_index])\n",
    "        _log.info(\"Get Factors\")          \n",
    "        self.normalizeFactor()\n",
    "        self.getFactors()\n",
    "        _log.info(\"Compute Item Similarity\")                    \n",
    "        self.computeItemSimilarity()\n",
    "        self.computeEntropy()\n",
    "        self.getMaxPatternForItem()\n",
    "        self.getMeanDistribution()\n",
    "        try:\n",
    "            self.getEmbedding()\n",
    "        except:\n",
    "            _log.info(\"running embedding again...\")\n",
    "            self.rd_state += 1\n",
    "            self.getEmbedding(rd_state = self.rd_state)\n",
    "        _log.info(\"Saving Output\")                              \n",
    "        self.formatOutput()\n",
    "        self.saveOutput()\n",
    "\n",
    "    def readFactorJSON(self, base_cnt=10, domain = \"\", ndims = 3):\n",
    "        self.base_cnt = base_cnt\n",
    "        self.domain = domain\n",
    "        self.ndims = ndims\n",
    "        file = \"../data/{}/factors_3_{}_sample_fit.json\".format(self.domain,self.ndims, self.base_cnt)\n",
    "        with open(file) as f:\n",
    "            self.data_output = json.load(f)\n",
    "\n",
    "    def readMetricJSON(self, base_cnt=10, domain = \"\", ndims = 3):\n",
    "        self.base_cnt = base_cnt\n",
    "        self.domain = domain\n",
    "        self.ndims = ndims\n",
    "        file = \"../data/{}/factors_{}_{}_sample_fit_metrics.json\".format(self.domain, self.ndims, self.base_cnt)\n",
    "        with open(file) as f:\n",
    "            metrics = json.load(f)\n",
    "        return metrics\n",
    "\n",
    "\n",
    "    def reEmbed(self, rd_state = 4):\n",
    "        self.getEmbedding(rd_state = rd_state)\n",
    "\n",
    "        for i in range(component_cnt):          \n",
    "            self.data_output['data'][i]['tsne_coord'] = {'x': self.X_embedded[i][0],'y':self.X_embedded[i][1]}\n",
    "            self.data_output['data'][i]['max_tsne'] = np.max(self.X_embedded, axis = 0).tolist()\n",
    "            self.data_output['data'][i]['min_tsne'] = np.min(self.X_embedded, axis = 0).tolist()\n",
    "\n",
    "        self.cur_base = component_cnt\n",
    "        self.column = self.data_output['modes']\n",
    "        self.saveOutput()\n",
    "\n",
    "\n",
    "def generateAll():\n",
    "    iFac = iFacData()\n",
    "    base = 30\n",
    "    iFac.start_index = 2\n",
    "    domain = \"policy\"   \n",
    "    nb_trials = 5\n",
    "\n",
    "    base = int(sys.argv[1])\n",
    "    iFac.start_index = int(sys.argv[2])\n",
    "    domain = str(sys.argv[3])\n",
    "\n",
    "    iFac.readData(domain = domain)\n",
    "    _log.info(\"Fitting Different Ranks up to {}\".format(base))\n",
    "    iFac.getFitForRanks(base, trials = nb_trials)\n",
    "\n",
    "def aggregateAll():\n",
    "    iFac = iFacData()\n",
    "    base = 30\n",
    "    iFac.start_index = 2\n",
    "    domain = \"policy\"   \n",
    "\n",
    "    iFac.start_index = int(sys.argv[1])\n",
    "    iFac.end_index = int(sys.argv[2])       \n",
    "    domain = str(sys.argv[3])\n",
    "        \n",
    "    start_metrics = iFac.readMetricJSON(base_cnt=iFac.start_index, domain = domain)\n",
    "    for i in range(iFac.start_index+1, iFac.end_index+1):\n",
    "        cur_metrics = iFac.readMetricJSON(base_cnt=i, domain = domain)\n",
    "        for m in measures:\n",
    "            cur_metrics[m] = [x for x in start_metrics[m] if x is not None] + [x for x in cur_metrics[m] if x is not None]        \n",
    "        with open('/home/xidao/project/thesis/iFac/src/src/data/'+domain+'/factors_3_'+str(i)+'_sample_fit_metrics.json', 'w') as fp:\n",
    "            json.dump(cur_metrics, fp)         \n",
    "\n",
    "def generateEmbedding():\n",
    "    return\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "# if __name__ == '__main__':\n",
    "#     generateAll()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/ipykernel_launcher.py:6: FutureWarning: \n",
      "Panel is deprecated and will be removed in a future version.\n",
      "The recommended way to represent these types of 3-dimensional data are with a MultiIndex on a DataFrame, via the Panel.to_frame() method\n",
      "Alternatively, you can use the xarray package http://xarray.pydata.org/en/stable/.\n",
      "Pandas provides a `.to_xarray()` method to help automate this conversion.\n",
      "\n",
      "  \n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-7c73a8b37782>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolumn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0meach_label\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpolicy_group1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfillna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maxes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0meach_label\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meach_one\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0meach_one\u001b[0m \u001b[0;32min\u001b[0m \u001b[0meach_label\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meach_label\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "policy = pd.read_csv(\"data/biomarker_gastric_complete.csv\")\n",
    "columns = ['age', 'gender','type', 'protein', 'value']\n",
    "policy.columns = columns\n",
    "column = columns[:4]\n",
    "policy_group = policy.groupby(column)['value'].sum()\n",
    "policy_group1 = policy_group.unstack(fill_value=0).to_panel()\n",
    "hist = policy_group1.fillna(0).values\n",
    "labels = []\n",
    "for i in range(len(column)):\n",
    "    each_label = policy_group1.fillna(0).axes[i].tolist()\n",
    "    each_label = [str(each_one) for each_one in each_label]\n",
    "    labels.append(each_label)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<class 'pandas.core.panel.Panel'>\n",
       "Dimensions: 312 (items) x 68 (major_axis) x 2 (minor_axis)\n",
       "Items axis: (3_hydroxybutyrate_dehydrogenase, gas_benign) to (squamous_cell_carcinoma_antigen, normal)\n",
       "Major_axis axis: 20 to 95\n",
       "Minor_axis axis: F to M"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "policy_group1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>gender</th>\n",
       "      <th>type</th>\n",
       "      <th>protein</th>\n",
       "      <th>value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>56</td>\n",
       "      <td>M</td>\n",
       "      <td>gas_benign</td>\n",
       "      <td>Neutrophil</td>\n",
       "      <td>0.431</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>56</td>\n",
       "      <td>M</td>\n",
       "      <td>gas_benign</td>\n",
       "      <td>Lymphocyte</td>\n",
       "      <td>0.439</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>56</td>\n",
       "      <td>M</td>\n",
       "      <td>gas_benign</td>\n",
       "      <td>Monocytes</td>\n",
       "      <td>0.072</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   age gender        type     protein  value\n",
       "0   56      M  gas_benign  Neutrophil  0.431\n",
       "1   56      M  gas_benign  Lymphocyte  0.439\n",
       "2   56      M  gas_benign   Monocytes  0.072"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "policy.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "iFac = iFacData()\n",
    "base = 6\n",
    "# iFac.start_index = \n",
    "# iFac.end_index = 40\n",
    "domain = \"harvard\"   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "iFac.readData(domain = domain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start factorization...\n",
      "* None\n",
      "* None\n",
      "* None\n",
      "* None\n",
      "* None\n",
      "* None\n",
      "* None\n",
      "* None\n",
      "* None\n",
      "* None\n",
      "* None\n",
      "* None\n",
      "* None\n",
      "* None\n",
      "* None\n",
      "* None\n",
      "* None\n",
      "* None\n",
      "* None\n",
      "[20/100]\n",
      " None\n",
      "* None\n",
      "* None\n",
      "* None\n",
      "* None\n",
      "* None\n",
      "* None\n",
      "* None\n",
      "* None\n",
      "* None\n",
      "* None\n",
      "* None\n",
      "* None\n",
      "* None\n",
      "* None\n",
      "* None\n",
      "* None\n",
      "* None\n",
      "* None\n",
      "* None\n",
      "[40/100]\n",
      " None\n",
      "* None\n",
      "* None\n",
      "* None\n",
      "* None\n",
      "* None\n",
      "* None\n",
      "* None\n",
      "* None\n",
      "* None\n",
      "* None\n",
      "* None\n",
      "* None\n",
      "* None\n",
      "* None\n",
      "* None\n",
      "* None\n",
      "* None\n",
      "* None\n",
      "* None\n",
      "[60/100]\n",
      " None\n",
      "* None\n",
      "* None\n",
      "* None\n",
      "* None\n",
      "* None\n",
      "* None\n",
      "* None\n",
      "* None\n",
      "* None\n",
      "* None\n",
      "* None\n",
      "* None\n",
      "* None\n",
      "* None\n",
      "* None\n",
      "* None\n",
      "* None\n",
      "* None\n",
      "* None\n",
      "[80/100]\n",
      " None\n",
      "* None\n",
      "* None\n",
      "* None\n",
      "* None\n",
      "* None\n",
      "* None\n",
      "* None\n",
      "* None\n",
      "* None\n",
      "* None\n",
      "* None\n",
      "* None\n",
      "* None\n",
      "* None\n",
      "* None\n",
      "* None\n",
      "* None\n",
      "* None\n",
      "* None\n",
      "[100/100]\n",
      " None\n"
     ]
    }
   ],
   "source": [
    "iFac.cur_base = base\n",
    "iFac.factorizeTensor(ones = False, random_seed = 1)\n",
    "iFac.normalizeFactor()\n",
    "iFac.getFactors()\n",
    "data = pd.DataFrame(iFac.data[0].T)\n",
    "data['student_id'] = iFac.labels[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data.to_csv(\"data/havard_vector\", sep=',', encoding='utf-8')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
