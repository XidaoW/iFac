{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark import SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "import sys\n",
    "conf = SparkConf()\n",
    "conf.set(\"spark.driver.memory\", \"100g\")\n",
    "conf.set(\"spark.executor.memory\", \"100g\")\n",
    "conf.set(\"spark.master\", \"local[30]\")\n",
    "conf.set(\"spark.driver.maxResultSize\", \"100g\")\n",
    "conf.set(\"spark.executor.heartbeatInterval\",\"1000000000s\")\n",
    "conf.set(\"spark.network.timeout\",\"1000000000s\")\n",
    "spark = SparkSession.builder.config(conf=conf).appName(\"convertProfile\").getOrCreate()\n",
    "\n",
    "from pyspark.ml.feature import HashingTF, IDF, Tokenizer\n",
    "from pyspark.ml.feature import Tokenizer, RegexTokenizer, CountVectorizer\n",
    "from pyspark.sql.functions import col, udf\n",
    "from pyspark.sql.types import IntegerType\n",
    "from pyspark.ml.feature import StopWordsRemover\n",
    "\n",
    "\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import types as T\n",
    "from pyspark.ml.linalg import SparseVector, DenseVector\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import lit, col, regexp_replace\n",
    "from pyspark.sql.functions import split, explode\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from pyspark.ml.feature import HashingTF, IDF, Tokenizer\n",
    "from pyspark.ml.feature import Tokenizer, RegexTokenizer, CountVectorizer\n",
    "from pyspark.sql.functions import col, udf\n",
    "from pyspark.sql.types import IntegerType\n",
    "\n",
    "\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import types as T\n",
    "from pyspark.ml.linalg import SparseVector, DenseVector\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import lit, col, regexp_replace\n",
    "from pyspark.sql.functions import split, explode\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class PokeProfile():\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        data can be downloaded at http://snap.stanford.edu/data/soc-Pokec.html\n",
    "        or run:\n",
    "        wget http://snap.stanford.edu/data/soc-pokec-relationships.txt.gz .\n",
    "        wget http://snap.stanford.edu/data/soc-pokec-profiles.txt.gz .\n",
    "        zcat soc-pokec-relationships.txt.gz > soc-pokec-relationships.txt\n",
    "        zcat soc-pokec-profiles.txt.gz > soc-pokec-profiles.txt                        \n",
    "        mkdir output # place to save output parquet\n",
    "        mkdir vocabulary # place to save vocabulary (vector-word mapping)\n",
    "        \"\"\"\n",
    "        self.userFile = \"data/seminar_year.csv\"\n",
    "        self.userFile = \"data/policy_adoption.csv\"\n",
    "        \n",
    "    def readProfile(self):\n",
    "        \"\"\"\n",
    "        read the profile data\n",
    "        \"\"\"\n",
    "        self.profiles_=spark.read.option(\"delimiter\", \",\").option(\"header\",\"true\").csv(self.userFile)\n",
    "        self.profiles_ = self.profiles_.na.fill(\"\")\n",
    "        \n",
    "\n",
    "    def formatHeaders(self, headers = [ \"user_id\",\"year\",\"title\"]):\n",
    "        \"\"\"\n",
    "        provide the headers of the data frame\n",
    "        \"\"\"\n",
    "        for c,n in zip(self.profiles_.columns,headers):\n",
    "            self.profiles_ = self.profiles_.withColumnRenamed(c,n)        \n",
    "            \n",
    "    def tokenize(self, inputColProfile = \"I_am_working_in_field\", vocSize = 40, minDF = 1.0):\n",
    "        \"\"\"\n",
    "        tokenize string column, count the occurence of words and then use the occurence of the top words as vector\n",
    "        :type inputColProfile: str: column to extract the vector\n",
    "        :type vocSize: int: number of words to count\n",
    "        :type minDF: float: minimun document frequency of the word            \n",
    "        :rtype: None\n",
    "\n",
    "        \"\"\"\n",
    "        self.vocSize = vocSize        \n",
    "        self.minDF = minDF\n",
    "        self.inputColProfile = inputColProfile\n",
    "        self.outputColProfile = \"{}_words\".format(self.inputColProfile)\n",
    "        self.outputColProfileStop = \"{}_words_stp\".format(self.inputColProfile)        \n",
    "        self.outputTokensColProfile = \"{}_tokens\".format(self.inputColProfile)\n",
    "        self.outputTokensDenseColProfile = \"{}_dense\".format(self.inputColProfile)\n",
    "\n",
    "        regexTokenizer = RegexTokenizer(inputCol=self.inputColProfile, outputCol=self.outputColProfile, pattern=\"\\\\W|\\\\d\")\n",
    "        self.profiles_ = regexTokenizer.transform(self.profiles_)\n",
    "\n",
    "        remover = StopWordsRemover(inputCol=self.outputColProfile, outputCol=self.outputColProfileStop)\n",
    "        self.profiles_ = remover.transform(self.profiles_)\n",
    "\n",
    "        self.cv = CountVectorizer(inputCol=self.outputColProfileStop, \n",
    "                             outputCol=self.outputTokensColProfile, \n",
    "                             vocabSize=self.vocSize, minDF=self.minDF)\n",
    "        try:\n",
    "            self.model = self.cv.fit(self.profiles_)        \n",
    "            self.profiles_ = self.model.transform(self.profiles_)        \n",
    "            vector_udf = udf(lambda vector: vector.toArray().tolist(),ArrayType(DoubleType()))\n",
    "\n",
    "            self.profiles_ = self.profiles_.withColumn(self.outputTokensDenseColProfile, vector_udf(self.outputTokensColProfile))\n",
    "            self.profiles_ = self.profiles_.drop(self.inputColProfile)\n",
    "            self.profiles_ = self.profiles_.drop(self.outputColProfile)\n",
    "            self.profiles_ = self.profiles_.drop(self.outputColProfileStop)            \n",
    "            self.profiles_ = self.profiles_.drop(self.outputTokensColProfile)\n",
    "        except:\n",
    "            print(\"Tokenizing {} Failed\".format(self.inputColProfile))\n",
    "            self.profiles_ = self.profiles_.drop(self.outputColProfile)\n",
    "            \n",
    "    def flattenVectorColumns(self, selected_columns = [\"user_id\", \"year\"]):\n",
    "        \"\"\"\n",
    "        convert from \n",
    "            col1=[0,1,2], col2=[0,1,2], col3=3, col4=0\n",
    "            to\n",
    "            col1.0,col1.1,col1.2,col2.0, col2.1,col2.2, col3, col4\n",
    "        \"\"\"\n",
    "        self.selected_columns = selected_columns\n",
    "        stringColumns = self.listStringColumns(self.index, self.cnt_each, all_columns = self.all_columns)        \n",
    "        stringColumns = [column + \"_dense\" for column in stringColumns]\n",
    "        self.newColumns = [self.profiles_[column][i] for column in stringColumns for i in range(self.vocSize)]\n",
    "        self.nonstringColumns = [column for column in self.profiles_.columns if column not in stringColumns]\n",
    "        self.profiles_flatten = self.profiles_.select(self.selected_columns + self.newColumns)\n",
    "#         self.profiles_flatten = self.profiles_.select(self.nonstringColumns + self.newColumns)        \n",
    "#         self.profiles_flatten = self.profiles_flatten.drop(\"_c59\")\n",
    "\n",
    "    def saveVocabulary(self):\n",
    "        \"\"\"\n",
    "        save the vocabulary to a separate file; \n",
    "        vocabulary can work as a look up table for the word given the index in the word vector        \n",
    "        \"\"\"\n",
    "        import pandas as pd\n",
    "        pd.DataFrame(self.model.vocabulary).to_csv(\"data/vocabulary/{}.txt\".format(self.inputColProfile), \n",
    "                                                   sep='\\t', encoding='utf-8', header=False)\n",
    "        \n",
    "    def listStringColumns(self, index = 0, cnt_each = 10, all_columns = [\"title\"]):\n",
    "        \"\"\"\n",
    "        list of string columns in the data\n",
    "        \"\"\"\n",
    "        self.index = index\n",
    "        self.cnt_each = cnt_each\n",
    "        self.all_columns = all_columns\n",
    "        # \"pets\", \"completed_level_of_education\", \"\"\n",
    "#         all_columns = [\"title\"]\n",
    "        self.cnt_string = len(all_columns) \n",
    "        start, end = index*cnt_each, (index+1)*cnt_each\n",
    "        return all_columns[start:end]\n",
    "    \n",
    "\n",
    "    def saveOutput(self, data, outputfile = \"soc-pokec-profiles-vector\", save_format = \"parquet\"):\n",
    "        \"\"\"\n",
    "        save data as parquet\n",
    "        \"\"\"\n",
    "        if save_format == \"parquet\":\n",
    "            data.repartition(1).write.parquet(\"{}.parquet\".format(outputfile))\n",
    "        else:\n",
    "            data.repartition(1).write.csv(\"{}.csv\".format(outputfile))\n",
    "\n",
    "from pyspark.sql.functions import array, col, explode, struct, lit\n",
    "\n",
    "df = sc.parallelize([(1, 0.0, 0.6), (1, 0.6, 0.7)]).toDF([\"A\", \"col_1\", \"col_2\"])\n",
    "\n",
    "def to_long(df, by):\n",
    "\n",
    "    # Filter dtypes and split into column names and type description\n",
    "    cols, dtypes = zip(*((c, t) for (c, t) in df.dtypes if c not in by))\n",
    "    # Spark SQL supports only homogeneous columns\n",
    "    assert len(set(dtypes)) == 1, \"All columns have to be of the same type\"\n",
    "\n",
    "    # Create and explode an array of (column_name, column_value) structs\n",
    "    kvs = explode(array([\n",
    "      struct(lit(c).alias(\"key\"), col(c).alias(\"val\")) for c in cols\n",
    "    ])).alias(\"kvs\")\n",
    "\n",
    "    return df.select(by + [kvs]).select(by + [\"kvs.key\", \"kvs.val\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "PP = PokeProfile()\n",
    "PP.readProfile()\n",
    "PP.formatHeaders()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in range(1):\n",
    "    print(i)\n",
    "    for eachColumn in PP.listStringColumns(i, cnt_each = 1):\n",
    "        PP.tokenize(inputColProfile = eachColumn, vocSize = 40)\n",
    "        PP.saveVocabulary()\n",
    "        PP.flattenVectorColumns()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "headers = [\"user_id\", \"year\"] + PP.model.vocabulary\n",
    "for c,n in zip(PP.profiles_flatten.columns,headers):\n",
    "    PP.profiles_flatten = PP.profiles_flatten.withColumnRenamed(c,n)        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(user_id=u'Ang', year=u'2018', networks=0.0, social=0.0, anomaly=0.0, twitter=0.0, dynamic=0.0, academic=0.0, hypersphere=0.0, conferences=0.0, detection=0.0, mobile=0.0, analysis=0.0, media=0.0, modeling=0.0, information=0.0, event=0.0, sentiment=0.0, unsupervised=0.0, beyond=1.0, robust=0.0, process=0.0, scale=0.0, march=1.0, feet=1.0, embedding=0.0, effect=0.0, discovery=0.0, method=0.0, protests=1.0, visual=0.0, seeking=0.0, data=0.0, role=0.0, large=0.0, analytics=0.0, talking=1.0, events=0.0, time=0.0, streaming=0.0, deep=0.0, regression=0.0),\n",
       " Row(user_id=u'Ang', year=u'2018', networks=0.0, social=0.0, anomaly=0.0, twitter=0.0, dynamic=0.0, academic=0.0, hypersphere=0.0, conferences=0.0, detection=0.0, mobile=0.0, analysis=0.0, media=0.0, modeling=0.0, information=0.0, event=0.0, sentiment=0.0, unsupervised=0.0, beyond=0.0, robust=0.0, process=0.0, scale=0.0, march=0.0, feet=0.0, embedding=0.0, effect=0.0, discovery=0.0, method=0.0, protests=0.0, visual=0.0, seeking=0.0, data=0.0, role=0.0, large=0.0, analytics=0.0, talking=0.0, events=1.0, time=0.0, streaming=0.0, deep=0.0, regression=0.0),\n",
       " Row(user_id=u'Ang', year=u'2018', networks=0.0, social=0.0, anomaly=0.0, twitter=0.0, dynamic=0.0, academic=0.0, hypersphere=0.0, conferences=0.0, detection=0.0, mobile=0.0, analysis=1.0, media=0.0, modeling=0.0, information=0.0, event=0.0, sentiment=1.0, unsupervised=0.0, beyond=0.0, robust=0.0, process=0.0, scale=0.0, march=0.0, feet=0.0, embedding=0.0, effect=0.0, discovery=0.0, method=0.0, protests=0.0, visual=0.0, seeking=0.0, data=0.0, role=0.0, large=0.0, analytics=0.0, talking=0.0, events=0.0, time=0.0, streaming=0.0, deep=0.0, regression=0.0)]"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PP.profiles_flatten = to_long(PP.profiles_flatten, [\"user_id\", \"year\"])\n",
    "PP.profiles_flatten = PP.profiles_flatten.repartition(1)\n",
    "PP.profiles_flatten.write.csv(\"data/scholar_top_{}.csv\".format(PP.vocSize), header = False, mode=\"overwrite\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "PP = PokeProfile()\n",
    "PP.readProfile()\n",
    "PP.formatHeaders(headers=[\"policy_id\",\"policy_name\",\"policy_subject_id\",\"policy_start\",\"policy_end\",\"policy_description\",\"policy_lda_1\",\"policy_lda_2\",\"policy_lda_3\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "PP.profiles_ = PP.profiles_.filter(PP.profiles_.policy_start >= 1970)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(policy_id=u'aborparc', policy_name=u'1-parent Consent for Abortion by a Minor', policy_subject_id=u'12', policy_start=u'1981', policy_end=u'1999', policy_description=u'1-parent Consent for Abortion by a Minor ', policy_lda_1=u'19', policy_lda_2=u'NULL', policy_lda_3=u'NULL'),\n",
       " Row(policy_id=u'aborparn', policy_name=u'1-parent Notification for Abortion by a Minor', policy_subject_id=u'12', policy_start=u'1981', policy_end=u'2000', policy_description=u'1-parent Notification for Abortion by a Minor ', policy_lda_1=u'19', policy_lda_2=u'NULL', policy_lda_3=u'NULL'),\n",
       " Row(policy_id=u'abortion_consent_1973_199', policy_name=u'Does The State Mandate Counseling Before An Abortion (Pre-Casey)?', policy_subject_id=u'2', policy_start=u'1973', policy_end=u'1991', policy_description=u'Does The State Mandate Counseling Before An Abortion (Pre-Casey)?', policy_lda_1=u'19', policy_lda_2=u'NULL', policy_lda_3=u'NULL')]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PP.profiles_.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['policy_name']"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PP.listStringColumns(i, cnt_each = 1, all_columns = [\"policy_name\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "for i in range(1):\n",
    "    print(i)\n",
    "    for eachColumn in PP.listStringColumns(i, cnt_each = 1, all_columns = [\"policy_name\"]):\n",
    "        PP.tokenize(inputColProfile = eachColumn, vocSize = 40)\n",
    "        PP.saveVocabulary()\n",
    "        PP.flattenVectorColumns(selected_columns = [\"policy_start\", \"policy_end\", \"policy_id\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "headers = PP.selected_columns + PP.model.vocabulary\n",
    "for c,n in zip(PP.profiles_flatten.columns,headers):\n",
    "    PP.profiles_flatten = PP.profiles_flatten.withColumnRenamed(c,n)        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(policy_start=u'1981', policy_end=u'1999', policy_id=u'aborparc', state=0.0, law=0.0, provides=0.0, ban=0.0, abortion=1.0, states=0.0, property=0.0, public=0.0, sex=0.0, regulates=0.0, child=0.0, insurance=0.0, laws=0.0, establishes=0.0, agreement=0.0, requires=0.0, management=0.0, notification=0.0, planning=0.0, tax=0.0, governs=0.0, credit=0.0, allows=0.0, system=0.0, legal=0.0, allow=0.0, limits=0.0, provide=0.0, coverage=0.0, level=0.0, offenders=0.0, use=0.0, abortions=0.0, health=0.0, electronic=0.0, real=0.0, act=0.0, rules=0.0, regulating=0.0, framework=0.0)]"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PP.profiles_flatten.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "PP.profiles_flatten = to_long(PP.profiles_flatten, PP.selected_columns)\n",
    "PP.profiles_flatten = PP.profiles_flatten.repartition(1)\n",
    "# PP.profiles_flatten.write.csv(\"data/policy_top_{}.csv\".format(PP.vocSize), header = False, mode=\"overwrite\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(policy_start=u'1981', policy_end=u'1999', policy_id=u'aborparc', key=u'state', val=0.0),\n",
       " Row(policy_start=u'1981', policy_end=u'1999', policy_id=u'aborparc', key=u'law', val=0.0),\n",
       " Row(policy_start=u'1981', policy_end=u'1999', policy_id=u'aborparc', key=u'provides', val=0.0)]"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PP.profiles_flatten.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "PP.profiles_state=spark.read.option(\"delimiter\", \",\").option(\"header\",\"true\").csv(\"data/policy_adoption_state.csv\")\n",
    "PP.profiles_state = PP.profiles_state.na.fill(\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(policy_id=u'corporateeff_yearadopted', adopted_year=u'1990', state_id=u'HI', subject_name=u'Macroeconomics'),\n",
       " Row(policy_id=u'corporateeff_yearadopted', adopted_year=u'1990', state_id=u'MA', subject_name=u'Macroeconomics'),\n",
       " Row(policy_id=u'corporateeff_yearadopted', adopted_year=u'1990', state_id=u'NC', subject_name=u'Macroeconomics')]"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PP.profiles_state.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "PP.profiles_all = PP.profiles_state.join(PP.profiles_flatten, PP.profiles_state.policy_id == PP.profiles_flatten.policy_id)\\\n",
    "                    .select(PP.profiles_state[\"*\"],PP.profiles_flatten[\"*\"]).drop(PP.profiles_state.policy_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(adopted_year=u'1990', state_id=u'HI', subject_name=u'Macroeconomics', policy_start=u'1990', policy_end=u'2008', policy_id=u'corporateeff_yearadopted', key=u'framework', val=0.0),\n",
       " Row(adopted_year=u'1990', state_id=u'HI', subject_name=u'Macroeconomics', policy_start=u'1990', policy_end=u'2008', policy_id=u'corporateeff_yearadopted', key=u'regulating', val=0.0),\n",
       " Row(adopted_year=u'1990', state_id=u'HI', subject_name=u'Macroeconomics', policy_start=u'1990', policy_end=u'2008', policy_id=u'corporateeff_yearadopted', key=u'rules', val=0.0),\n",
       " Row(adopted_year=u'1990', state_id=u'HI', subject_name=u'Macroeconomics', policy_start=u'1990', policy_end=u'2008', policy_id=u'corporateeff_yearadopted', key=u'act', val=0.0),\n",
       " Row(adopted_year=u'1990', state_id=u'HI', subject_name=u'Macroeconomics', policy_start=u'1990', policy_end=u'2008', policy_id=u'corporateeff_yearadopted', key=u'real', val=0.0)]"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PP.profiles_all.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "PP.profiles_all = PP.profiles_all.repartition(1)\n",
    "PP.profiles_all.write.csv(\"data/policy_top_{}.csv\".format(PP.vocSize), header = True, mode=\"overwrite\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# self = PP\n",
    "# picso = pd.read_csv(\"data/picso.csv\", header=None)\n",
    "# picso.columns = ['member', 'year', 'keyword', 'value']\n",
    "# # policy_group = policy.groupby(self.column)['adoption'].sum()\n",
    "# # policy_group1 = policy_group.unstack(fill_value=0).to_panel()\n",
    "# # self.hist = policy_group1.fillna(0).values"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pySpark (Spark 2.0.0)",
   "language": "python",
   "name": "pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
